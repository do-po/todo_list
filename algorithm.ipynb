{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 알고리즘\n",
    "\n",
    "- 유저의 행동데이터를 받아와서 기초 알고리즘의 가중치들 수정\n",
    "- 메인 알고리즘을 작성하여 유저들의 속성과 그 행동 데이터들 만으로 가중치 설정\n",
    "    - 유저의 향후 행동 데이터를 계속 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 행동 데이터 (유저 선택의 결과)\n",
    "\n",
    "- schedule_2\n",
    "- 유저가 일정을 변경하는 버튼을 따로 만들고, 이 버튼(일정 확정)이 푸시 되었을 때와 그 전을 비교해서 가중치에 수정을 가한다\n",
    "- 유저가 자유롭게 일정을 드래그 & 드롭으로 수정할 수 있게 만들고, 수정된 것에 대한 가중치를 수집한다 (이런 경우엔 세션 종료 시 변경 여부를 가져와서 가중치 수정을 가하는 방법이겠지 ??)\n",
    "- todo를 루틴과 루틴이 아닌 것으로 구분\n",
    "\n",
    "- 유저가 많이 수행하는 todo의 카테고리를 잡고, 해당 카테고리에서 벗어나는 카테고리를 주의 카테고리로 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 랜덤 포레스트 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 초기 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = './refer/output/'\n",
    "\n",
    "# 1. 초기 모델 생성용 데이터 로드\n",
    "# 사용자 정보 데이터를 CSV 파일에서 불러옵니다.\n",
    "user_info = pd.read_csv(f'{file_path}random_user_info.csv')\n",
    "\n",
    "# 초기 y 값 데이터를 CSV 파일에서 불러옵니다.\n",
    "schedule_1 = pd.read_csv(f'{file_path}random_schedule_1.csv')\n",
    "\n",
    "# base_weight 데이터를 CSV 파일에서 불러옵니다.\n",
    "base_weight = pd.read_csv(f'{file_path}base_weight.csv')\n",
    "\n",
    "# 2. 초기 y 값 설정\n",
    "# schedule_1의 base_weight_no와 base_weight의 no를 매칭하여 필요한 열을 가져옵니다.\n",
    "y_df = schedule_1[['base_weight_no']].merge(base_weight, left_on='base_weight_no', right_on='no')[['work', 'edu', 'free_time', 'health', 'chores', 'category_else']]\n",
    "\n",
    "# 3. 데이터 분할\n",
    "# X 변수는 사용자 정보의 특정 열들을 선택합니다.\n",
    "X = user_info[['age', 'gender', 'mbti', 'job']]\n",
    "# 데이터를 학습용과 테스트용으로 분할합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. 원-핫 인코딩 및 전처리 파이프라인 설정\n",
    "# 범주형 변수를 원-핫 인코딩합니다.\n",
    "categorical_features = ['gender', 'mbti', 'job']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# ColumnTransformer를 사용해 범주형 변수 전처리 및 나머지 변수는 그대로 유지합니다.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# 5. 모델 파이프라인 구성 및 하이퍼파라미터 조정\n",
    "# 파라미터 그리드를 설정합니다.\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200, 300],  # 트리의 개수, 더 많은 트리는 더 나은 성능을 제공할 수 있지만 훈련 시간이 늘어납니다.\n",
    "    'regressor__max_features': ['auto', 'sqrt', 'log2'],  # 각 노드에서 고려할 최대 특성 수, 'auto'는 모든 특성을 사용함을 의미합니다.\n",
    "    'regressor__max_depth': [10, 20, 30, None],  # 트리의 최대 깊이, None은 무제한 깊이를 의미합니다.\n",
    "    'regressor__min_samples_split': [2, 5, 10],  # 내부 노드를 분할하는 데 필요한 최소 샘플 수, 값이 클수록 모델이 더 일반화됩니다.\n",
    "    'regressor__min_samples_leaf': [1, 2, 4]  # 리프 노드에 있어야 하는 최소 샘플 수, 값이 클수록 모델이 더 일반화됩니다.\n",
    "}\n",
    "\n",
    "# 파이프라인을 사용하여 전처리 및 모델을 설정합니다.\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# GridSearchCV를 사용하여 하이퍼파라미터 튜닝을 수행합니다.\n",
    "grid_search = GridSearchCV(\n",
    "    model,\n",
    "    param_grid,import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = './refer/output/'\n",
    "\n",
    "# 1. 초기 모델 생성용 데이터 로드\n",
    "# 사용자 정보 데이터를 CSV 파일에서 불러옵니다.\n",
    "user_info = pd.read_csv(f'{file_path}random_user_info.csv')\n",
    "\n",
    "# 초기 y 값 데이터를 CSV 파일에서 불러옵니다.\n",
    "schedule_1 = pd.read_csv(f'{file_path}random_schedule_1.csv')\n",
    "\n",
    "# base_weight 데이터를 CSV 파일에서 불러옵니다.\n",
    "base_weight = pd.read_csv(f'{file_path}base_weight.csv')\n",
    "\n",
    "# 2. 초기 y 값 설정\n",
    "# schedule_1의 base_weight_no와 base_weight의 no를 매칭하여 필요한 열을 가져옵니다.\n",
    "y_df = schedule_1[['base_weight_no']].merge(base_weight, left_on='base_weight_no', right_on='no')[['work', 'edu', 'free_time', 'health', 'chores', 'category_else']]\n",
    "\n",
    "# 3. 데이터 분할\n",
    "# X 변수는 사용자 정보의 특정 열들을 선택합니다.\n",
    "X = user_info[['age', 'gender', 'mbti', 'job']]\n",
    "# 데이터를 학습용과 테스트용으로 분할합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. 원-핫 인코딩 및 전처리 파이프라인 설정\n",
    "# 범주형 변수를 원-핫 인코딩합니다.\n",
    "categorical_features = ['gender', 'mbti', 'job']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# ColumnTransformer를 사용해 범주형 변수 전처리 및 나머지 변수는 그대로 유지합니다.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# 5. 모델 파이프라인 구성 및 하이퍼파라미터 조정\n",
    "# 파라미터 그리드를 설정합니다.\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200, 300],  # 트리의 개수, 더 많은 트리는 더 나은 성능을 제공할 수 있지만 훈련 시간이 늘어납니다.\n",
    "    'regressor__max_features': ['sqrt', 'log2', None],  # 각 노드에서 고려할 최대 특성 수, None은 모든 특성을 사용함을 의미합니다.\n",
    "    'regressor__max_depth': [10, 20, 30, None],  # 트리의 최대 깊이, None은 무제한 깊이를 의미합니다.\n",
    "    'regressor__min_samples_split': [2, 5, 10],  # 내부 노드를 분할하는 데 필요한 최소 샘플 수, 값이 클수록 모델이 더 일반화됩니다.\n",
    "    'regressor__min_samples_leaf': [1, 2, 4]  # 리프 노드에 있어야 하는 최소 샘플 수, 값이 클수록 모델이 더 일반화됩니다.\n",
    "}\n",
    "\n",
    "# 파이프라인을 사용하여 전처리 및 모델을 설정합니다.\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# GridSearchCV를 사용하여 하이퍼파라미터 튜닝을 수행합니다.\n",
    "grid_search = GridSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    cv=3,  # 교차 검증 폴드 수\n",
    "    n_jobs=-1,  # 사용할 CPU 코어 수, -1은 모든 코어 사용\n",
    "    scoring='neg_mean_squared_error'  # 평가 기준, 여기서는 음의 평균 제곱 오차\n",
    ")\n",
    "\n",
    "# 모델을 학습합니다.\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 모델을 저장합니다.\n",
    "best_model = grid_search.best_estimator_\n",
    "joblib.dump(best_model, f'{file_path}initial_model.pkl')\n",
    "\n",
    "print(\"Initial model created and saved.\")\n",
    "\n",
    "    cv=3,  # 교차 검증 폴드 수\n",
    "    n_jobs=-1,  # 사용할 CPU 코어 수, -1은 모든 코어 사용\n",
    "    scoring='neg_mean_squared_error'  # 평가 기준, 여기서는 음의 평균 제곱 오차\n",
    ")\n",
    "\n",
    "# 모델을 학습합니다.\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 모델을 저장합니다.\n",
    "best_model = grid_search.best_estimator_\n",
    "joblib.dump(best_model, f'{file_path}initial_model.pkl')\n",
    "\n",
    "print(\"Initial model created and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = './refer/output/'\n",
    "\n",
    "# 1. 기존 모델 로드\n",
    "# 초기 모델을 파일에서 로드합니다.\n",
    "model = joblib.load(f'{file_path}initial_model.pkl')\n",
    "\n",
    "# 2. schedule_2에서 업데이트 데이터 로드\n",
    "# schedule_2에서 최신 데이터를 불러옵니다.\n",
    "schedule_2 = pd.read_csv(f'{file_path}schedule_2.csv')\n",
    "\n",
    "# 3. user_info에서 사용자 정보 데이터 로드\n",
    "# 사용자 정보 데이터를 CSV 파일에서 불러옵니다.\n",
    "user_info = pd.read_csv(f'{file_path}random_user_info.csv')\n",
    "\n",
    "# 4. 업데이트할 X, y 설정\n",
    "# X 업데이트 데이터를 설정합니다.\n",
    "X_update = user_info[['age', 'gender', 'mbti', 'job']]\n",
    "# y 업데이트 데이터를 설정합니다.\n",
    "y_update = schedule_2[['work', 'edu', 'free_time', 'health', 'chores', 'category_else']].iloc[-1].values.reshape(1, -1)\n",
    "\n",
    "# 5. 원-핫 인코딩 및 전처리 파이프라인 설정\n",
    "# 범주형 변수를 원-핫 인코딩합니다.\n",
    "categorical_features = ['gender', 'mbti', 'job']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# ColumnTransformer를 사용해 범주형 변수 전처리 및 나머지 변수는 그대로 유지합니다.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# 6. 파이프라인을 사용하여 전처리 및 모델 설정\n",
    "# 모델의 전처리 부분을 업데이트합니다.\n",
    "model.steps[0] = ('preprocessor', preprocessor)\n",
    "\n",
    "# 7. 모델 업데이트\n",
    "# X 업데이트 데이터를 전처리합니다.\n",
    "X_train_update = preprocessor.fit_transform(X_update)\n",
    "# 모델을 업데이트된 데이터로 재학습시킵니다.\n",
    "model.named_steps['regressor'].fit(X_train_update, y_update)\n",
    "\n",
    "# 8. 업데이트된 모델 저장\n",
    "# 업데이트된 모델을 저장합니다.\n",
    "joblib.dump(model, f'{file_path}updated_model.pkl')\n",
    "\n",
    "print(\"Model updated and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Not for Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가독성을 위해 지수 표현(e) 변환\n",
    "pd.options.display.float_format = '{:.5f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './refer/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로딩\n",
    "data = pd.read_csv(f'{file_path}survey_data.csv')\n",
    "\n",
    "# 고객 선택 데이터\n",
    "user_schedule = pd.read_csv(f'{file_path}user_schedule.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 데이터를 원-핫 인코딩\n",
    "categorical_features = ['gender', 'mbti', 'job']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# 전처리 파이프라인 설정\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# 타겟 변수와 피처 분리\n",
    "X = data[['age', 'gender', 'mbti', 'job']]\n",
    "y = data[['work', 'edu', 'free_time', 'health', 'chores']]\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 모델 파이프라인 구성\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 예측 결과를 데이터프레임으로 변환\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=y_test.columns)\n",
    "\n",
    "# MSE 계산 결과를 데이터프레임으로 변환\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse_df = pd.DataFrame([mse], columns=['MSE'])\n",
    "\n",
    "# 결과 출력\n",
    "print(mse_df)\n",
    "print(y_pred_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그라디언트 부스팅 회귀 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM 빌드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 초기 모델 빌드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from lightgbm import LGBMRegressor\n",
    "import joblib\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = 'your/file/path/'\n",
    "\n",
    "# 1. 초기 모델 생성용 데이터 로드\n",
    "# 사용자 정보 데이터를 CSV 파일에서 불러옵니다.\n",
    "user_info = pd.read_csv(f'{file_path}user_info.csv')\n",
    "\n",
    "# 초기 y 값 데이터를 CSV 파일에서 불러옵니다.\n",
    "schedule_1 = pd.read_csv(f'{file_path}schedule_1.csv')\n",
    "\n",
    "# 2. 초기 y 값 설정\n",
    "# 초기 y 값으로 schedule_1 데이터를 사용합니다.\n",
    "y_df = schedule_1.copy()\n",
    "\n",
    "# 3. 데이터 분할\n",
    "# X 변수는 사용자 정보의 특정 열들을 선택합니다.\n",
    "X = user_info[['age', 'gender', 'mbti', 'job']]\n",
    "# 데이터를 학습용과 테스트용으로 분할합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. 원-핫 인코딩 및 전처리 파이프라인 설정\n",
    "# 범주형 변수를 원-핫 인코딩합니다.\n",
    "categorical_features = ['gender', 'mbti', 'job']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# ColumnTransformer를 사용해 범주형 변수 전처리 및 나머지 변수는 그대로 유지합니다.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# 5. 각 타겟에 대한 모델 훈련 및 저장\n",
    "# 하이퍼파라미터 그리드를 설정합니다.\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # 부스팅 반복 횟수, 더 많은 반복은 더 나은 성능을 제공할 수 있지만, 훈련 시간이 늘어납니다.\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # 학습률, 모델이 업데이트되는 속도를 조절합니다. 작은 값은 더 천천히 학습하지만 더 나은 일반화를 제공합니다.\n",
    "    'num_leaves': [31, 50, 100],  # 하나의 트리가 가질 수 있는 최대 잎사귀 수, 값이 클수록 모델의 복잡도가 높아집니다.\n",
    "    'max_depth': [-1, 10, 20, 30],  # 트리의 최대 깊이, -1은 무제한 깊이를 의미합니다.\n",
    "    'min_child_samples': [20, 50, 100],  # 리프 노드가 되기 위한 최소 데이터 수, 값이 클수록 모델이 더 일반화됩니다.\n",
    "    'feature_fraction': [0.6, 0.8, 1.0],  # 각 트리마다 사용되는 피처 비율, 0.8은 각 트리가 무작위로 선택된 80%의 피처를 사용함을 의미합니다.\n",
    "    'bagging_fraction': [0.6, 0.8, 1.0],  # 데이터 샘플링 비율, 0.8은 각 반복에서 무작위로 선택된 80%의 데이터를 사용함을 의미합니다.\n",
    "    'bagging_freq': [0, 5, 10],  # 배깅 빈도, k번의 반복 후 데이터 샘플링을 수행합니다. 0은 사용하지 않음을 의미합니다.\n",
    "    'lambda_l1': [0, 0.1, 1.0],  # L1 정규화, 모델의 가중치를 0으로 만들도록 합니다. 과적합을 방지합니다.\n",
    "    'lambda_l2': [0, 0.1, 1.0]  # L2 정규화, 모델의 가중치를 작게 유지하도록 합니다. 과적합을 방지합니다.\n",
    "}\n",
    "\n",
    "# 각 타겟에 대해 모델을 훈련하고 최적의 하이퍼파라미터를 찾습니다.\n",
    "models = {}\n",
    "for target in y_df.columns:\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', LGBMRegressor(random_state=42))\n",
    "    ])\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=3,  # 교차 검증 폴드 수\n",
    "        n_jobs=-1,  # 사용할 CPU 코어 수, -1은 모든 코어 사용\n",
    "        scoring='neg_mean_squared_error'  # 평가 기준, 여기서는 음의 평균 제곱 오차\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train[target])\n",
    "    models[target] = grid_search.best_estimator_\n",
    "\n",
    "# 6. 모델 저장\n",
    "joblib.dump(models, f'{file_path}initial_lightgbm_models.pkl')\n",
    "\n",
    "print(\"Initial LightGBM models created and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from lightgbm import LGBMRegressor\n",
    "import joblib\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = 'your/file/path/'\n",
    "\n",
    "# 1. 기존 모델 로드\n",
    "# 초기 모델과 전처리기를 파일에서 로드합니다.\n",
    "models = joblib.load(f'{file_path}initial_lightgbm_models.pkl')\n",
    "\n",
    "# 2. schedule_2에서 업데이트 데이터 로드\n",
    "# schedule_2에서 최신 데이터를 불러옵니다.\n",
    "schedule_2 = pd.read_csv(f'{file_path}schedule_2.csv')\n",
    "\n",
    "# 3. user_info에서 사용자 정보 데이터 로드\n",
    "# 사용자 정보 데이터를 CSV 파일에서 불러옵니다.\n",
    "user_info = pd.read_csv(f'{file_path}user_info.csv')\n",
    "\n",
    "# 4. 업데이트할 X, y 설정\n",
    "# X 업데이트 데이터를 설정합니다.\n",
    "X_update = user_info[['age', 'gender', 'mbti', 'job']]\n",
    "# y 업데이트 데이터를 설정합니다.\n",
    "y_update = schedule_2.iloc[-1].values.reshape(1, -1)\n",
    "\n",
    "# 5. 전처리\n",
    "# 범주형 변수를 원-핫 인코딩합니다.\n",
    "categorical_features = ['gender', 'mbti', 'job']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# ColumnTransformer를 사용해 범주형 변수 전처리 및 나머지 변수는 그대로 유지합니다.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# 업데이트 데이터에 대해 전처리를 수행합니다.\n",
    "X_update = preprocessor.fit_transform(X_update)\n",
    "\n",
    "# 6. 모델 업데이트\n",
    "for idx, target in enumerate(schedule_2.columns):\n",
    "    models[target].named_steps['regressor'].fit(X_update, y_update[:, idx])\n",
    "\n",
    "# 7. 업데이트된 모델 저장\n",
    "joblib.dump(models, f'{file_path}updated_lightgbm_models.pkl')\n",
    "\n",
    "print(\"LightGBM models updated and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Not for Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 로드\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로딩\n",
    "data = pd.read_csv(f'{file_path}survey_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 데이터를 원-핫 인코딩\n",
    "categorical_features = ['gender', 'mbti', 'job']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# 전처리 파이프라인 설정\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# 타겟 변수와 피처 분리\n",
    "X = data[['age', 'gender', 'mbti', 'job']]\n",
    "y = data[['work', 'edu', 'free_time', 'health', 'chores']]\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 전처리\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# 각 타겟에 대한 모델 훈련 및 예측\n",
    "mse = []\n",
    "predictions = []\n",
    "for target in y.columns:\n",
    "    model = LGBMRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train[target])\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse.append(mean_squared_error(y_test[target], y_pred))\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "# 예측 결과를 데이터프레임으로 변환\n",
    "y_pred_df = pd.DataFrame(zip(*predictions), columns=y.columns)\n",
    "\n",
    "# MSE 결과 출력\n",
    "for idx, target in enumerate(y.columns):\n",
    "    print(f'MSE for {target}: {format(mse[idx], \".4f\")}')\n",
    "\n",
    "# 전체 MSE 계산 및 출력\n",
    "overall_mse = mean_squared_error(y_test, y_pred_df)\n",
    "print(\"Overall MSE:\", format(overall_mse, \".4f\"))\n",
    "\n",
    "# 예측 결과 데이터프레임 출력, 지수 표현식 없이\n",
    "print(y_pred_df.to_string(index=False, header=True, float_format=\"{:0.4f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 코드 적용해서 하이퍼 파라미터 수정 필요\n",
    "'''\n",
    "for target in y.columns:\n",
    "    model = LGBMRegressor(\n",
    "        n_estimators=200,  # 트리의 수 증가\n",
    "        random_state=42,\n",
    "        max_depth=10,  # 트리의 최대 깊이 증가\n",
    "        min_data_in_leaf=20,  # 한 리프가 가지는 최소 데이터 수\n",
    "        learning_rate=0.05  # 학습률 감소\n",
    "    )\n",
    "    model.fit(X_train, y_train[target])\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse.append(mean_squared_error(y_test[target], y_pred))\n",
    "    predictions.append(y_pred)\n",
    "    print(f'MSE for {target}: {format(mean_squared_error(y_test[target], y_pred), \".4f\")}')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost 빌드 (너무 메모리 많이 먹고 오래 걸려서 사용 안함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 초기 모델 빌드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = 'your/file/path/'\n",
    "\n",
    "# 1. 초기 모델 생성용 데이터 로드\n",
    "# 사용자 정보 데이터를 CSV 파일에서 불러옵니다.\n",
    "user_info = pd.read_csv(f'{file_path}user_info.csv')\n",
    "\n",
    "# 초기 y 값 데이터를 CSV 파일에서 불러옵니다.\n",
    "schedule_1 = pd.read_csv(f'{file_path}schedule_1.csv')\n",
    "\n",
    "# 2. 초기 y 값 설정\n",
    "# 초기 y 값으로 schedule_1 데이터를 사용합니다.\n",
    "y_df = schedule_1.copy()\n",
    "\n",
    "# 3. 데이터 분할\n",
    "# X 변수는 사용자 정보의 특정 열들을 선택합니다.\n",
    "X = user_info[['age', 'gender', 'mbti', 'job']]\n",
    "# 데이터를 학습용과 테스트용으로 분할합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. 원-핫 인코딩 및 전처리 파이프라인 설정\n",
    "# 범주형 변수를 원-핫 인코딩합니다.\n",
    "categorical_features = ['gender', 'mbti', 'job']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# ColumnTransformer를 사용해 범주형 변수 전처리 및 나머지 변수는 그대로 유지합니다.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# 5. 전처리\n",
    "# 학습 데이터에 대해 전처리를 수행합니다.\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# 6. 각 타겟에 대한 모델 훈련 및 저장\n",
    "# 하이퍼파라미터 그리드를 설정합니다.\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # 부스팅 반복 횟수\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # 학습률\n",
    "    'max_depth': [3, 5, 7],  # 트리의 최대 깊이\n",
    "    'min_child_weight': [1, 3, 5],  # 자식 노드를 가지기 위한 최소 가중치 합\n",
    "    'subsample': [0.6, 0.8, 1.0],  # 각 트리마다 사용되는 데이터 샘플 비율\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]  # 각 트리마다 사용되는 피처 샘플 비율\n",
    "}\n",
    "\n",
    "# 각 타겟에 대해 모델을 훈련하고 최적의 하이퍼파라미터를 찾습니다.\n",
    "models = {}\n",
    "for target in y_df.columns:\n",
    "    model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train[target])\n",
    "    models[target] = grid_search.best_estimator_\n",
    "\n",
    "# 7. 모델 저장\n",
    "joblib.dump(models, f'{file_path}initial_xgboost_models.pkl')\n",
    "joblib.dump(preprocessor, f'{file_path}preprocessor.pkl')\n",
    "\n",
    "print(\"Initial XGBoost models created and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = 'your/file/path/'\n",
    "\n",
    "# 1. 기존 모델 로드\n",
    "# 초기 모델과 전처리기를 파일에서 로드합니다.\n",
    "models = joblib.load(f'{file_path}initial_xgboost_models.pkl')\n",
    "preprocessor = joblib.load(f'{file_path}preprocessor.pkl')\n",
    "\n",
    "# 2. schedule_2에서 업데이트 데이터 로드\n",
    "# schedule_2에서 최신 데이터를 불러옵니다.\n",
    "schedule_2 = pd.read_csv(f'{file_path}schedule_2.csv')\n",
    "\n",
    "# 3. user_info에서 사용자 정보 데이터 로드\n",
    "# 사용자 정보 데이터를 CSV 파일에서 불러옵니다.\n",
    "user_info = pd.read_csv(f'{file_path}user_info.csv')\n",
    "\n",
    "# 4. 업데이트할 X, y 설정\n",
    "# X 업데이트 데이터를 설정합니다.\n",
    "X_update = user_info[['age', 'gender', 'mbti', 'job']]\n",
    "# y 업데이트 데이터를 설정합니다.\n",
    "y_update = schedule_2.iloc[-1].values.reshape(1, -1)\n",
    "\n",
    "# 5. 전처리\n",
    "# 업데이트 데이터에 대해 전처리를 수행합니다.\n",
    "X_update = preprocessor.transform(X_update)\n",
    "\n",
    "# 6. 모델 업데이트\n",
    "for idx, target in enumerate(schedule_2.columns):\n",
    "    models[target].fit(X_update, y_update[:, idx])\n",
    "\n",
    "# 7. 업데이트된 모델 저장\n",
    "joblib.dump(models, f'{file_path}updated_xgboost_models.pkl')\n",
    "\n",
    "print(\"XGBoost models updated and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Not for Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 로드\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로딩\n",
    "data = pd.read_csv(f'{file_path}survey_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 데이터를 원-핫 인코딩\n",
    "categorical_features = ['gender', 'mbti', 'job']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# 전처리 파이프라인 설정\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# 타겟 변수와 피처 분리\n",
    "X = data[['age', 'gender', 'mbti', 'job']]\n",
    "y = data[['work', 'edu', 'free_time', 'health', 'chores']]\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 전처리 및 변환\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# 각 타겟에 대한 모델 훈련 및 예측 결과 저장\n",
    "predictions = []\n",
    "for i, target in enumerate(y.columns):\n",
    "    model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train.iloc[:, i])\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions.append(y_pred)\n",
    "    print(f'MSE for {target}: {mean_squared_error(y_test.iloc[:, i], y_pred)}')\n",
    "\n",
    "# 예측 결과를 데이터프레임으로 변환\n",
    "y_pred_df = pd.DataFrame(predictions).T  # Transpose to align with y_test's shape\n",
    "y_pred_df.columns = y_test.columns\n",
    "\n",
    "# 예측 데이터프레임과 실제 데이터프레임의 MSE 계산\n",
    "mse = mean_squared_error(y_test, y_pred_df)\n",
    "print(\"Overall MSE:\", format(mse, \".4f\"))\n",
    "\n",
    "# 예측 결과 데이터프레임 출력, 지수 표현식 없이\n",
    "print(y_pred_df.to_string(index=False, header=True, float_format=\"{:0.4f}\".format))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝 모델 (시간 없어서 사용 안함 (데이터도 없어))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow 빌드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로딩\n",
    "data = pd.read_csv(f'{file_path}survey_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 데이터를 원-핫 인코딩\n",
    "categorical_features = ['gender', 'mbti', 'job']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# 전처리 파이프라인 설정\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# 타겟 변수와 피처 분리\n",
    "X = data[['age', 'gender', 'mbti', 'job']]\n",
    "y = data[['work', 'edu', 'free_time', 'health', 'chores']]\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 전처리 및 변환\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 모델 구성\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_dim=X_train.shape[1]),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(5)  # 출력 레이어: 5개 타겟 변수\n",
    "])\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 및 평가\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# MSE 계산\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "mse_value = mse(y_test, y_pred).numpy()\n",
    "print(\"MSE:\", format(mse_value, \".4f\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시간 슬라이싱\n",
    "\n",
    "- 딥러닝 모델로 목표에 대한 업무를 중요도와 긴급도를 기준으로 시간 슬라이싱\n",
    "- 중요도(weight), 긴급도(fire_data, complexity)를 통해 가용 시간 하루 시간 단위로 슬라이싱\n",
    "- 슬라이싱 된 작은 목표 달성시 계속, 미 달성시(부분 미달성, 전체 미달성) 슬라이싱에 다시 반영"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow 빌드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 작업 중요도 예측 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "file_path = '/mnt/data/survey_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data = data.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# 범주형 데이터에 대한 원-핫 인코딩\n",
    "encoder = OneHotEncoder()\n",
    "encoded_data = encoder.fit_transform(data[['gender', 'job', 'mbti']]).toarray()\n",
    "\n",
    "# 인코딩된 데이터와 수치형 데이터를 결합\n",
    "numeric_data = data[['age', 'work', 'edu', 'free_time', 'health', 'chores']].values\n",
    "X = np.hstack((encoded_data, numeric_data))\n",
    "\n",
    "# 데이터 정규화\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# 목표 변수 정의 (중요도)\n",
    "y = data[['work', 'edu', 'free_time', 'health', 'chores']].values\n",
    "\n",
    "# 데이터를 학습용과 테스트용으로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "learning_rate = 0.001  # 학습률: 학습 과정에서 가중치가 조정되는 속도를 결정합니다.\n",
    "batch_size = 32       # 배치 크기: 한 번의 훈련 반복에서 사용되는 샘플의 수입니다.\n",
    "epochs = 100          # 에포크 수: 전체 데이터셋을 훈련하는 반복 횟수입니다.\n",
    "hidden_layer1_size = 128  # 첫 번째 은닉층의 노드 수: 모델의 복잡성을 조절합니다.\n",
    "hidden_layer2_size = 64   # 두 번째 은닉층의 노드 수: 모델의 복잡성을 조절합니다.\n",
    "\n",
    "# TensorFlow/Keras를 사용하여 신경망 모델 정의\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_layer1_size, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(hidden_layer2_size, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1]))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "# 모델 평가\n",
    "train_loss = model.evaluate(X_train, y_train)\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(f'훈련 손실: {train_loss:.4f}')\n",
    "print(f'테스트 손실: {test_loss:.4f}')\n",
    "\n",
    "# 예측 및 평가 (평균 제곱 오차 사용)\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "print(f'훈련 MSE: {train_mse:.4f}')\n",
    "print(f'테스트 MSE: {test_mse:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 작업 긴급도 예측 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시를 위한 start_date, end_date 및 complexity에 대한 가상의 데이터 생성\n",
    "np.random.seed(42)\n",
    "data['start_date'] = np.random.randint(0, 30, size=len(data))\n",
    "data['end_date'] = data['start_date'] + np.random.randint(1, 10, size=len(data))\n",
    "data['complexity'] = np.random.randint(1, 11, size=len(data))\n",
    "\n",
    "# 긴급도 모델을 위한 입력 데이터 준비\n",
    "X_urgency = data[['start_date', 'end_date', 'complexity']].values\n",
    "y_urgency = np.random.rand(len(data), 1)  # 예시를 위한 가상의 긴급도 데이터\n",
    "\n",
    "# 데이터 정규화\n",
    "scaler = StandardScaler()\n",
    "X_urgency = scaler.fit_transform(X_urgency)\n",
    "\n",
    "# 데이터를 학습용과 테스트용으로 분할\n",
    "X_train_urgency, X_test_urgency, y_train_urgency, y_test_urgency = train_test_split(X_urgency, y_urgency, test_size=0.2, random_state=42)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "learning_rate_urgency = 0.001  # 학습률: 학습 과정에서 가중치가 조정되는 속도를 결정합니다.\n",
    "batch_size_urgency = 32       # 배치 크기: 한 번의 훈련 반복에서 사용되는 샘플의 수입니다.\n",
    "epochs_urgency = 100          # 에포크 수: 전체 데이터셋을 훈련하는 반복 횟수입니다.\n",
    "hidden_layer1_size_urgency = 128  # 첫 번째 은닉층의 노드 수: 모델의 복잡성을 조절합니다.\n",
    "hidden_layer2_size_urgency = 64   # 두 번째 은닉층의 노드 수: 모델의 복잡성을 조절합니다.\n",
    "\n",
    "# TensorFlow/Keras를 사용하여 긴급도 예측을 위한 신경망 모델 정의\n",
    "urgency_model = Sequential()\n",
    "urgency_model.add(Dense(hidden_layer1_size_urgency, input_dim=X_train_urgency.shape[1], activation='relu'))\n",
    "urgency_model.add(Dense(hidden_layer2_size_urgency, activation='relu'))\n",
    "urgency_model.add(Dense(1))\n",
    "\n",
    "# 모델 컴파일\n",
    "urgency_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_urgency), loss='mse')\n",
    "\n",
    "# 모델 학습\n",
    "urgency_model.fit(X_train_urgency, y_train_urgency, epochs=epochs_urgency, batch_size=batch_size_urgency, validation_split=0.2)\n",
    "\n",
    "# 모델 평가\n",
    "train_loss_urgency = urgency_model.evaluate(X_train_urgency, y_train_urgency)\n",
    "test_loss_urgency = urgency_model.evaluate(X_test_urgency, y_test_urgency)\n",
    "print(f'훈련 긴급도 손실: {train_loss_urgency:.4f}')\n",
    "print(f'테스트 긴급도 손실: {test_loss_urgency:.4f}')\n",
    "\n",
    "# 예측 및 평가 (평균 제곱 오차 사용)\n",
    "y_pred_train_urgency = urgency_model.predict(X_train_urgency)\n",
    "y_pred_test_urgency = urgency_model.predict(X_test_urgency)\n",
    "train_mse_urgency = mean_squared_error(y_train_urgency, y_pred_train_urgency)\n",
    "test_mse_urgency = mean_squared_error(y_test_urgency, y_pred_test_urgency)\n",
    "print(f'훈련 긴급도 MSE: {train_mse_urgency:.4f}')\n",
    "print(f'테스트 긴급도 MSE: {test_mse_urgency:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
