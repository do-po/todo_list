{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 알고리즘\n",
    "\n",
    "- 유저의 행동데이터를 받아와서 기초 알고리즘의 가중치들 수정\n",
    "- 메인 알고리즘을 작성하여 유저들의 속성과 그 행동 데이터들 만으로 가중치 설정\n",
    "    - 유저의 향후 행동 데이터를 계속 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 행동 데이터 (유저 선택의 결과)\n",
    "\n",
    "- schedule_2\n",
    "- 유저가 일정을 변경하는 버튼을 따로 만들고, 이 버튼(일정 확정)이 푸시 되었을 때와 그 전을 비교해서 가중치에 수정을 가한다\n",
    "- 유저가 자유롭게 일정을 드래그 & 드롭으로 수정할 수 있게 만들고, 수정된 것에 대한 가중치를 수집한다 (이런 경우엔 세션 종료 시 변경 여부를 가져와서 가중치 수정을 가하는 방법이겠지 ??)\n",
    "- todo를 루틴과 루틴이 아닌 것으로 구분\n",
    "\n",
    "- 유저가 많이 수행하는 todo의 카테고리를 잡고, 해당 카테고리에서 벗어나는 카테고리를 주의 카테고리로 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 랜덤 포레스트 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 초기 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model created and saved.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = './refer/output/'\n",
    "\n",
    "# 1. 초기 모델 생성용 데이터 로드\n",
    "# 사용자 정보 데이터를 CSV 파일에서 불러옵니다.\n",
    "user_info = pd.read_csv(f'{file_path}random_user_info.csv')\n",
    "\n",
    "# 초기 y 값 데이터를 CSV 파일에서 불러옵니다.\n",
    "schedule_1 = pd.read_csv(f'{file_path}random_schedule_1.csv')\n",
    "\n",
    "# base_weight 데이터를 CSV 파일에서 불러옵니다.\n",
    "base_weight = pd.read_csv(f'{file_path}base_weight.csv')\n",
    "\n",
    "# 2. 초기 y 값 설정\n",
    "# schedule_1의 base_weight_no와 base_weight의 no를 매칭하여 필요한 열을 가져옵니다.\n",
    "y_df = schedule_1[['base_weight_no']].merge(base_weight, left_on='base_weight_no', right_on='no')[['work', 'edu', 'free_time', 'health', 'chores', 'category_else']]\n",
    "\n",
    "# 3. 데이터 분할\n",
    "# X 변수는 사용자 정보의 특정 열들을 선택합니다.\n",
    "X = user_info[['age', 'gender', 'mbti', 'job']]\n",
    "# 데이터를 학습용과 테스트용으로 분할합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. 원-핫 인코딩 및 전처리 파이프라인 설정\n",
    "# 범주형 변수를 원-핫 인코딩합니다.\n",
    "categorical_features = ['gender', 'mbti', 'job']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# ColumnTransformer를 사용해 범주형 변수 전처리 및 나머지 변수는 그대로 유지합니다.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# 5. 모델 파이프라인 구성 및 하이퍼파라미터 조정\n",
    "# 파라미터 그리드를 설정합니다.\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200, 300],  # 트리의 개수, 더 많은 트리는 더 나은 성능을 제공할 수 있지만 훈련 시간이 늘어납니다.\n",
    "    'regressor__max_features': ['sqrt', 'log2', None],  # 각 노드에서 고려할 최대 특성 수, None은 모든 특성을 사용함을 의미합니다.\n",
    "    'regressor__max_depth': [10, 20, 30, None],  # 트리의 최대 깊이, None은 무제한 깊이를 의미합니다.\n",
    "    'regressor__min_samples_split': [2, 5, 10],  # 내부 노드를 분할하는 데 필요한 최소 샘플 수, 값이 클수록 모델이 더 일반화됩니다.\n",
    "    'regressor__min_samples_leaf': [1, 2, 4]  # 리프 노드에 있어야 하는 최소 샘플 수, 값이 클수록 모델이 더 일반화됩니다.\n",
    "}\n",
    "\n",
    "# 파이프라인을 사용하여 전처리 및 모델을 설정합니다.\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# GridSearchCV를 사용하여 하이퍼파라미터 튜닝을 수행합니다.\n",
    "grid_search = GridSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    cv=3,  # 교차 검증 폴드 수\n",
    "    n_jobs=-1,  # 사용할 CPU 코어 수, -1은 모든 코어 사용\n",
    "    scoring='neg_mean_squared_error'  # 평가 기준, 여기서는 음의 평균 제곱 오차\n",
    ")\n",
    "\n",
    "# 모델을 학습합니다.\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 모델을 저장합니다.\n",
    "best_model = grid_search.best_estimator_\n",
    "joblib.dump(best_model, f'{file_path}initial_model.pkl')\n",
    "\n",
    "print(\"Initial model created and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 후기 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "from joblib import parallel_backend\n",
    "\n",
    "# 디버깅 메시지 출력 함수\n",
    "def debug_message(message):\n",
    "    print(f\"[디버깅] {message}\")\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = './refer/output/'\n",
    "\n",
    "# 1. 초기 모델 생성용 데이터 로드\n",
    "debug_message(\"사용자 정보 데이터를 불러오는 중...\")\n",
    "user_info = pd.read_csv(f'{file_path}random_user_info.csv')\n",
    "debug_message(\"사용자 정보 데이터 로드 완료\")\n",
    "\n",
    "debug_message(\"schedule_2 데이터를 불러오는 중...\")\n",
    "schedule_2 = pd.read_csv(f'{file_path}random_schedule_2.csv')\n",
    "debug_message(\"schedule_2 데이터 로드 완료\")\n",
    "\n",
    "debug_message(\"user_weight 데이터를 불러오는 중...\")\n",
    "user_weight = pd.read_csv(f'{file_path}random_user_weight.csv')\n",
    "debug_message(\"user_weight 데이터 로드 완료\")\n",
    "\n",
    "# 2. y 값 설정\n",
    "debug_message(\"y 값 설정 중...\")\n",
    "y_df = schedule_2[['user_weight_no']].merge(user_weight, on='user_weight_no')[['work', 'edu', 'free_time', 'health', 'chores', 'category_else']]\n",
    "debug_message(\"y 값 설정 완료\")\n",
    "\n",
    "# 3. 데이터 분할\n",
    "debug_message(\"데이터 분할 중...\")\n",
    "X = user_info[['age', 'gender', 'mbti', 'job']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_df, test_size=0.2, random_state=42)\n",
    "debug_message(\"데이터 분할 완료\")\n",
    "\n",
    "# 4. 원-핫 인코딩 및 전처리 파이프라인 설정\n",
    "debug_message(\"원-핫 인코딩 및 전처리 파이프라인 설정 중...\")\n",
    "categorical_features = ['gender', 'mbti', 'job']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)  # 'cat'은 범주형 변수 원-핫 인코딩을 수행하는 변환기\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "debug_message(\"원-핫 인코딩 및 전처리 파이프라인 설정 완료\")\n",
    "\n",
    "# 5. 모델 파이프라인 구성 및 하이퍼파라미터 조정\n",
    "debug_message(\"하이퍼파라미터 그리드 설정 중...\")\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200, 300],  # 트리의 개수, 더 많은 트리는 더 나은 성능을 제공할 수 있지만 훈련 시간이 늘어남\n",
    "    'regressor__max_features': ['sqrt', 'log2', None],  # 각 노드에서 고려할 최대 특성 수, None은 모든 특성을 사용함을 의미\n",
    "    'regressor__max_depth': [10, 20, 30, None],  # 트리의 최대 깊이, None은 무제한 깊이를 의미\n",
    "    'regressor__min_samples_split': [2, 5, 10],  # 내부 노드를 분할하는 데 필요한 최소 샘플 수, 값이 클수록 모델이 더 일반화됨\n",
    "    'regressor__min_samples_leaf': [1, 2, 4]  # 리프 노드에 있어야 하는 최소 샘플 수, 값이 클수록 모델이 더 일반화됨\n",
    "}\n",
    "debug_message(\"하이퍼파라미터 그리드 설정 완료\")\n",
    "\n",
    "# 파이프라인을 사용하여 전처리 및 모델을 설정합니다.\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# GridSearchCV를 사용하여 하이퍼파라미터 튜닝을 수행합니다.\n",
    "with parallel_backend('threading'):\n",
    "    grid_search = GridSearchCV(\n",
    "        model,\n",
    "        param_grid,\n",
    "        cv=3,  # 교차 검증 폴드 수\n",
    "        n_jobs=-1,  # 사용할 CPU 코어 수, -1은 모든 코어 사용\n",
    "        scoring='neg_mean_squared_error'  # 평가 기준, 여기서는 음의 평균 제곱 오차\n",
    "    )\n",
    "\n",
    "    # 모델을 학습합니다.\n",
    "    debug_message(\"모델 학습 중...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    debug_message(\"모델 학습 완료\")\n",
    "\n",
    "# 최적의 모델을 저장합니다.\n",
    "best_model = grid_search.best_estimator_\n",
    "joblib.dump(best_model, f'{file_path}initial_model.pkl')\n",
    "\n",
    "debug_message(\"최적의 모델 저장 완료\")\n",
    "print(\"Initial model created and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './refer/output/schedule_2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124minitial_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 2. schedule_2에서 업데이트 데이터 로드\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# schedule_2에서 최신 데이터를 불러옵니다.\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m schedule_2 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mschedule_2.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 3. user_info에서 사용자 정보 데이터 로드\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 사용자 정보 데이터를 CSV 파일에서 불러옵니다.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m user_info \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mrandom_user_info.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './refer/output/schedule_2.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = './refer/output/'\n",
    "\n",
    "# 1. 기존 모델 로드\n",
    "# 초기 모델을 파일에서 로드합니다.\n",
    "model = joblib.load(f'{file_path}initial_model.pkl')\n",
    "\n",
    "# 2. schedule_2에서 업데이트 데이터 로드\n",
    "# schedule_2에서 최신 데이터를 불러옵니다.\n",
    "schedule_2 = pd.read_csv(f'{file_path}random_schedule_2.csv')\n",
    "\n",
    "# 3. user_info에서 사용자 정보 데이터 로드\n",
    "# 사용자 정보 데이터를 CSV 파일에서 불러옵니다.\n",
    "user_info = pd.read_csv(f'{file_path}random_user_info.csv')\n",
    "\n",
    "# 4. 업데이트할 X, y 설정\n",
    "# X 업데이트 데이터를 설정합니다.\n",
    "X_update = user_info[['age', 'gender', 'mbti', 'job']]\n",
    "# y 업데이트 데이터를 설정합니다.\n",
    "y_update = schedule_2.iloc[-1].values.reshape(1, -1)\n",
    "\n",
    "# 5. 원-핫 인코딩 및 전처리 파이프라인 설정\n",
    "# 범주형 변수를 원-핫 인코딩합니다.\n",
    "categorical_features = ['gender', 'mbti', 'job']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# ColumnTransformer를 사용해 범주형 변수 전처리 및 나머지 변수는 그대로 유지합니다.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# 6. 파이프라인을 사용하여 전처리 및 모델 설정\n",
    "# 모델의 전처리 부분을 업데이트합니다.\n",
    "model.steps[0] = ('preprocessor', preprocessor)\n",
    "\n",
    "# 7. 모델 업데이트\n",
    "# X 업데이트 데이터를 전처리합니다.\n",
    "X_train_update = preprocessor.fit_transform(X_update)\n",
    "# 모델을 업데이트된 데이터로 재학습시킵니다.\n",
    "model.named_steps['regressor'].fit(X_train_update, y_update)\n",
    "\n",
    "# 8. 업데이트된 모델 저장\n",
    "# 업데이트된 모델을 저장합니다.\n",
    "joblib.dump(model, f'{file_path}updated_model.pkl')\n",
    "\n",
    "print(\"Model updated and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 후기 업데이트 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "\n",
    "# 디버깅 메시지 출력 함수\n",
    "def debug_message(message):\n",
    "    print(f\"[디버깅] {message}\")\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = './refer/output/'\n",
    "\n",
    "# 1. 기존 모델 및 데이터 로드\n",
    "debug_message(\"기존 모델과 데이터를 로드하는 중...\")\n",
    "model = joblib.load(f'{file_path}initial_model.pkl')\n",
    "user_info = pd.read_csv(f'{file_path}random_user_info.csv')\n",
    "schedule_2 = pd.read_csv(f'{file_path}random_schedule_2.csv')\n",
    "user_weight = pd.read_csv(f'{file_path}random_user_weight.csv')\n",
    "debug_message(\"기존 모델과 데이터 로드 완료\")\n",
    "\n",
    "# 데이터프레임의 열 이름 출력\n",
    "debug_message(f\"user_weight 열 이름: {user_weight.columns}\")\n",
    "debug_message(f\"schedule_2 열 이름: {schedule_2.columns}\")\n",
    "\n",
    "# 2. y 값 설정\n",
    "debug_message(\"y 값 설정 중...\")\n",
    "y_df = schedule_2[['user_weight_no']].merge(user_weight, on='user_weight_no')[['work', 'edu', 'free_time', 'health', 'chores', 'category_else']]\n",
    "debug_message(\"y 값 설정 완료\")\n",
    "\n",
    "# 3. 데이터 결합\n",
    "debug_message(\"데이터 결합 중...\")\n",
    "X_combined = user_info.copy()\n",
    "y_combined = y_df.copy()\n",
    "debug_message(\"데이터 결합 완료\")\n",
    "\n",
    "# 4. 원-핫 인코딩 및 전처리 파이프라인 설정\n",
    "debug_message(\"원-핫 인코딩 및 전처리 파이프라인 설정 중...\")\n",
    "categorical_features = ['gender', 'mbti', 'job']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)  # 'cat'은 범주형 변수 원-핫 인코딩을 수행하는 변환기\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "debug_message(\"원-핫 인코딩 및 전처리 파이프라인 설정 완료\")\n",
    "\n",
    "# 5. 모델 파이프라인 설정 및 재학습\n",
    "debug_message(\"모델 재학습 중...\")\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# 모델을 전체 데이터를 사용하여 재학습합니다.\n",
    "model.fit(X_combined, y_combined)\n",
    "debug_message(\"모델 재학습 완료\")\n",
    "\n",
    "# 6. 업데이트된 모델 저장\n",
    "joblib.dump(model, f'{file_path}updated_model.pkl')\n",
    "\n",
    "debug_message(\"업데이트된 모델 저장 완료\")\n",
    "print(\"Random Forest model updated and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Not for Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가독성을 위해 지수 표현(e) 변환\n",
    "pd.options.display.float_format = '{:.5f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './refer/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로딩\n",
    "data = pd.read_csv(f'{file_path}survey_data.csv')\n",
    "\n",
    "# 고객 선택 데이터\n",
    "user_schedule = pd.read_csv(f'{file_path}user_schedule.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 데이터를 원-핫 인코딩\n",
    "categorical_features = ['gender', 'mbti', 'job']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# 전처리 파이프라인 설정\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# 타겟 변수와 피처 분리\n",
    "X = data[['age', 'gender', 'mbti', 'job']]\n",
    "y = data[['work', 'edu', 'free_time', 'health', 'chores']]\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 모델 파이프라인 구성\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 예측 결과를 데이터프레임으로 변환\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=y_test.columns)\n",
    "\n",
    "# MSE 계산 결과를 데이터프레임으로 변환\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse_df = pd.DataFrame([mse], columns=['MSE'])\n",
    "\n",
    "# 결과 출력\n",
    "print(mse_df)\n",
    "print(y_pred_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그라디언트 부스팅 회귀 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM 빌드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 초기 모델 빌드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[디버깅] 사용자 정보 데이터를 불러오는 중...\n",
      "[디버깅] 사용자 정보 데이터 로드 완료\n",
      "[디버깅] 초기 y 값 데이터를 불러오는 중...\n",
      "[디버깅] 초기 y 값 데이터 로드 완료\n",
      "[디버깅] base_weight 데이터를 불러오는 중...\n",
      "[디버깅] base_weight 데이터 로드 완료\n",
      "[디버깅] schedule_2 데이터를 불러오는 중...\n",
      "[디버깅] schedule_2 데이터 로드 완료\n",
      "[디버깅] user_weight 데이터를 불러오는 중...\n",
      "[디버깅] user_weight 데이터 로드 완료\n",
      "[디버깅] user_weight 열 이름: Index(['user_weight_no', 'work', 'edu', 'free_time', 'health', 'chores',\n",
      "       'category_else', 'history_no'],\n",
      "      dtype='object')\n",
      "[디버깅] schedule_2 열 이름: Index(['user_id', 'user_goal', 'goal_complexity', 'goal_start_date',\n",
      "       'goal_end_date', 'record_time', 'user_weight_no'],\n",
      "      dtype='object')\n",
      "[디버깅] y 값 설정 중...\n",
      "[디버깅] y 값 설정 완료\n",
      "[디버깅] 데이터 분할 중...\n",
      "[디버깅] 데이터 분할 완료\n",
      "[디버깅] 원-핫 인코딩 및 전처리 파이프라인 설정 중...\n",
      "[디버깅] 원-핫 인코딩 및 전처리 파이프라인 설정 완료\n",
      "[디버깅] 하이퍼파라미터 그리드 설정 중...\n",
      "[디버깅] 하이퍼파라미터 그리드 설정 완료\n",
      "[디버깅] work에 대한 모델 훈련 중...\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000942 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 197\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 29\n",
      "[LightGBM] [Info] Start training from score 0.501396\n",
      "[디버깅] work에 대한 모델 훈련 완료\n",
      "[디버깅] edu에 대한 모델 훈련 중...\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000579 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 197\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 29\n",
      "[LightGBM] [Info] Start training from score 0.500739\n",
      "[디버깅] edu에 대한 모델 훈련 완료\n",
      "[디버깅] free_time에 대한 모델 훈련 중...\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 197\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 29\n",
      "[LightGBM] [Info] Start training from score 0.499775\n",
      "[디버깅] free_time에 대한 모델 훈련 완료\n",
      "[디버깅] health에 대한 모델 훈련 중...\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000791 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 197\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 29\n",
      "[LightGBM] [Info] Start training from score 0.500183\n",
      "[디버깅] health에 대한 모델 훈련 완료\n",
      "[디버깅] chores에 대한 모델 훈련 중...\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 197\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 29\n",
      "[LightGBM] [Info] Start training from score 0.498949\n",
      "[디버깅] chores에 대한 모델 훈련 완료\n",
      "[디버깅] 모델 저장 중...\n",
      "[디버깅] 모델 저장 완료: initial_lightgbm_models_v2.pkl\n",
      "Initial LightGBM models created and saved as initial_lightgbm_models_v2.pkl.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from lightgbm import LGBMRegressor\n",
    "import joblib\n",
    "from joblib import parallel_backend\n",
    "\n",
    "# 디버깅 메시지 출력 함수\n",
    "def debug_message(message):\n",
    "    print(f\"[디버깅] {message}\")\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = './refer/output/'\n",
    "\n",
    "# 1. 초기 모델 생성용 데이터 로드\n",
    "debug_message(\"사용자 정보 데이터를 불러오는 중...\")\n",
    "user_info = pd.read_csv(f'{file_path}random_user_info.csv')\n",
    "debug_message(\"사용자 정보 데이터 로드 완료\")\n",
    "\n",
    "debug_message(\"초기 y 값 데이터를 불러오는 중...\")\n",
    "schedule_1 = pd.read_csv(f'{file_path}random_schedule_1.csv')\n",
    "debug_message(\"초기 y 값 데이터 로드 완료\")\n",
    "\n",
    "debug_message(\"base_weight 데이터를 불러오는 중...\")\n",
    "base_weight = pd.read_csv(f'{file_path}base_weight.csv')\n",
    "debug_message(\"base_weight 데이터 로드 완료\")\n",
    "\n",
    "debug_message(\"schedule_2 데이터를 불러오는 중...\")\n",
    "schedule_2 = pd.read_csv(f'{file_path}random_schedule_2.csv')\n",
    "debug_message(\"schedule_2 데이터 로드 완료\")\n",
    "\n",
    "debug_message(\"user_weight 데이터를 불러오는 중...\")\n",
    "user_weight = pd.read_csv(f'{file_path}random_user_weight.csv')\n",
    "debug_message(\"user_weight 데이터 로드 완료\")\n",
    "\n",
    "# 데이터프레임의 열 이름 출력\n",
    "debug_message(f\"user_weight 열 이름: {user_weight.columns}\")\n",
    "debug_message(f\"schedule_2 열 이름: {schedule_2.columns}\")\n",
    "\n",
    "# 2. y 값 설정\n",
    "debug_message(\"y 값 설정 중...\")\n",
    "y_df = schedule_2[['user_weight_no']].merge(\n",
    "    user_weight, on='user_weight_no'\n",
    ")[['work', 'edu', 'free_time', 'health', 'chores']]\n",
    "debug_message(\"y 값 설정 완료\")\n",
    "\n",
    "# 3. 데이터 분할\n",
    "debug_message(\"데이터 분할 중...\")\n",
    "X = user_info[['age', 'gender', 'mbti', 'job']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_df, test_size=0.2, random_state=42)\n",
    "debug_message(\"데이터 분할 완료\")\n",
    "\n",
    "# 4. 원-핫 인코딩 및 전처리 파이프라인 설정\n",
    "debug_message(\"원-핫 인코딩 및 전처리 파이프라인 설정 중...\")\n",
    "categorical_features = ['gender', 'mbti', 'job']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)  # 'cat'은 범주형 변수 원-핫 인코딩을 수행하는 변환기\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "debug_message(\"원-핫 인코딩 및 전처리 파이프라인 설정 완료\")\n",
    "\n",
    "# 5. 각 타겟에 대한 모델 훈련 및 저장\n",
    "debug_message(\"하이퍼파라미터 그리드 설정 중...\")\n",
    "\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200, 300],  # 부스팅 반복 횟수, 더 많은 반복은 더 나은 성능을 제공할 수 있지만, 훈련 시간 증가\n",
    "    'regressor__learning_rate': [0.01, 0.05, 0.1],  # 학습률, 모델이 업데이트되는 속도 조절, 작은 값은 더 천천히 학습하지만 더 나은 일반화 제공\n",
    "    'regressor__num_leaves': [31, 50, 100],  # 하나의 트리가 가질 수 있는 최대 잎사귀 수, 값이 클수록 모델의 복잡도 증가\n",
    "    'regressor__max_depth': [-1, 10, 20, 30],  # 트리의 최대 깊이, -1은 무제한 깊이를 의미\n",
    "    'regressor__min_child_samples': [20, 50, 100],  # 리프 노드가 되기 위한 최소 데이터 수, 값이 클수록 모델이 더 일반화됨\n",
    "    'regressor__feature_fraction': [0.6, 0.8, 1.0],  # 각 트리마다 사용되는 피처 비율, 0.8은 각 트리가 무작위로 선택된 80%의 피처를 사용함을 의미\n",
    "    'regressor__bagging_fraction': [0.6, 0.8, 1.0],  # 데이터 샘플링 비율, 0.8은 각 반복에서 무작위로 선택된 80%의 데이터를 사용함을 의미\n",
    "    'regressor__bagging_freq': [0, 5, 10],  # 배깅 빈도, k번의 반복 후 데이터 샘플링을 수행, 0은 사용하지 않음을 의미\n",
    "    'regressor__lambda_l1': [0, 0.1, 1.0],  # L1 정규화, 모델의 가중치를 0으로 만들도록 함, 과적합 방지\n",
    "    'regressor__lambda_l2': [0, 0.1, 1.0]  # L2 정규화, 모델의 가중치를 작게 유지하도록 함, 과적합 방지\n",
    "}\n",
    "\n",
    "\n",
    "debug_message(\"하이퍼파라미터 그리드 설정 완료\")\n",
    "\n",
    "models = {}\n",
    "# parallel_backend를 사용하여 threading 백엔드로 설정\n",
    "# 멀티프로세싱 대신 멀티스레딩을 사용하여 피클링 문제 회피\n",
    "with parallel_backend('threading'):\n",
    "    for target in y_df.columns:\n",
    "        debug_message(f\"{target}에 대한 모델 훈련 중...\")\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', LGBMRegressor(random_state=42))\n",
    "        ])\n",
    "        grid_search = GridSearchCV(\n",
    "            pipeline,\n",
    "            param_grid,\n",
    "            cv=3,  # 교차 검증 폴드 수\n",
    "            n_jobs=-1,  # 사용할 CPU 코어 수, -1은 모든 코어 사용\n",
    "            scoring='neg_mean_squared_error'  # 평가 기준, 음의 평균 제곱 오차 사용\n",
    "        )\n",
    "        grid_search.fit(X_train, y_train[target])\n",
    "        models[target] = grid_search.best_estimator_\n",
    "        debug_message(f\"{target}에 대한 모델 훈련 완료\")\n",
    "\n",
    "# 6. 모델 저장 함수\n",
    "def save_model_with_versioning(models, file_path, base_filename):\n",
    "    version = 1\n",
    "    filename = f\"{base_filename}_v{version}.pkl\"\n",
    "    while os.path.exists(f\"{file_path}{filename}\"):\n",
    "        version += 1\n",
    "        filename = f\"{base_filename}_v{version}.pkl\"\n",
    "    joblib.dump(models, f\"{file_path}{filename}\")\n",
    "    return filename\n",
    "\n",
    "# 모델 저장\n",
    "debug_message(\"모델 저장 중...\")\n",
    "saved_filename = save_model_with_versioning(models, file_path, \"initial_lightgbm_models\")\n",
    "debug_message(f\"모델 저장 완료: {saved_filename}\")\n",
    "\n",
    "print(f\"Initial LightGBM models created and saved as {saved_filename}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 피클링 오류 발생했던 기존 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 각 타겟에 대한 모델 훈련 및 저장\n",
    "# 하이퍼파라미터 그리드를 설정합니다.\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # 부스팅 반복 횟수, 더 많은 반복은 더 나은 성능을 제공할 수 있지만, 훈련 시간이 늘어납니다.\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # 학습률, 모델이 업데이트되는 속도를 조절합니다. 작은 값은 더 천천히 학습하지만 더 나은 일반화를 제공합니다.\n",
    "    'num_leaves': [31, 50, 100],  # 하나의 트리가 가질 수 있는 최대 잎사귀 수, 값이 클수록 모델의 복잡도가 높아집니다.\n",
    "    'max_depth': [-1, 10, 20, 30],  # 트리의 최대 깊이, -1은 무제한 깊이를 의미합니다.\n",
    "    'min_child_samples': [20, 50, 100],  # 리프 노드가 되기 위한 최소 데이터 수, 값이 클수록 모델이 더 일반화됩니다.\n",
    "    'feature_fraction': [0.6, 0.8, 1.0],  # 각 트리마다 사용되는 피처 비율, 0.8은 각 트리가 무작위로 선택된 80%의 피처를 사용함을 의미합니다.\n",
    "    'bagging_fraction': [0.6, 0.8, 1.0],  # 데이터 샘플링 비율, 0.8은 각 반복에서 무작위로 선택된 80%의 데이터를 사용함을 의미합니다.\n",
    "    'bagging_freq': [0, 5, 10],  # 배깅 빈도, k번의 반복 후 데이터 샘플링을 수행합니다. 0은 사용하지 않음을 의미합니다.\n",
    "    'lambda_l1': [0, 0.1, 1.0],  # L1 정규화, 모델의 가중치를 0으로 만들도록 합니다. 과적합을 방지합니다.\n",
    "    'lambda_l2': [0, 0.1, 1.0]  # L2 정규화, 모델의 가중치를 작게 유지하도록 합니다. 과적합을 방지합니다.\n",
    "}\n",
    "\n",
    "# 각 타겟에 대해 모델을 훈련하고 최적의 하이퍼파라미터를 찾습니다.\n",
    "models = {}\n",
    "for target in y_df.columns:\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', LGBMRegressor(random_state=42))\n",
    "    ])\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=3,  # 교차 검증 폴드 수\n",
    "        n_jobs=-1,  # 사용할 CPU 코어 수, -1은 모든 코어 사용\n",
    "        scoring='neg_mean_squared_error'  # 평가 기준, 여기서는 음의 평균 제곱 오차\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train[target])\n",
    "    models[target] = grid_search.best_estimator_\n",
    "\n",
    "# 6. 모델 저장\n",
    "joblib.dump(models, f'{file_path}initial_lightgbm_models.pkl') # joblib의 기본 설정인 Loky 사용으로 인해 피클링 오류 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from lightgbm import LGBMRegressor\n",
    "import joblib\n",
    "\n",
    "# 디버깅 메시지 출력 함수\n",
    "def debug_message(message):\n",
    "    print(f\"[디버깅] {message}\")\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = './refer/output/'\n",
    "\n",
    "# 기존 모델 및 데이터 로드\n",
    "debug_message(\"기존 모델과 데이터를 로드하는 중...\")\n",
    "models = joblib.load(f'{file_path}initial_lightgbm_models.pkl')\n",
    "user_info = pd.read_csv(f'{file_path}random_user_info.csv')\n",
    "schedule_1 = pd.read_csv(f'{file_path}random_schedule_1.csv')\n",
    "base_weight = pd.read_csv(f'{file_path}base_weight.csv')\n",
    "schedule_2 = pd.read_csv(f'{file_path}random_schedule_2.csv')\n",
    "user_weight = pd.read_csv(f'{file_path}random_user_weight.csv')\n",
    "debug_message(\"기존 모델과 데이터 로드 완료\")\n",
    "\n",
    "# 초기 y 값 설정 및 결합\n",
    "debug_message(\"초기 y 값 설정 및 결합 중...\")\n",
    "y_df_1 = schedule_1[['base_weight_no']].merge(base_weight, left_on='base_weight_no', right_on='no')[['work', 'edu', 'free_time', 'health', 'chores']]\n",
    "y_df_2 = schedule_2[['user_weight_no']].merge(user_weight, on='user_weight_no')[['work', 'edu', 'free_time', 'health', 'chores']]\n",
    "debug_message(\"초기 y 값 설정 및 결합 완료\")\n",
    "\n",
    "# X 및 y 결합\n",
    "debug_message(\"X 및 y 결합 중...\")\n",
    "X_combined = pd.concat([user_info, user_info])\n",
    "y_combined = pd.concat([y_df_1, y_df_2])\n",
    "debug_message(\"X 및 y 결합 완료\")\n",
    "\n",
    "# 원-핫 인코딩 및 전처리 파이프라인 설정\n",
    "debug_message(\"원-핫 인코딩 및 전처리 파이프라인 설정 중...\")\n",
    "categorical_features = ['gender', 'mbti', 'job']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)  # 'cat'은 범주형 변수 원-핫 인코딩을 수행하는 변환기\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "debug_message(\"원-핫 인코딩 및 전처리 파이프라인 설정 완료\")\n",
    "\n",
    "# 파이프라인 설정 및 모델 재학습\n",
    "debug_message(\"모델 재학습 중...\")\n",
    "for target in y_combined.columns:\n",
    "    debug_message(f\"{target}에 대한 모델 훈련 중...\")\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', LGBMRegressor(random_state=42))\n",
    "    ])\n",
    "    pipeline.fit(X_combined, y_combined[target])\n",
    "    models[target] = pipeline\n",
    "    debug_message(f\"{target}에 대한 모델 훈련 완료\")\n",
    "\n",
    "# 모델 저장 함수\n",
    "def save_model_with_versioning(models, file_path, base_filename):\n",
    "    version = 1\n",
    "    filename = f\"{base_filename}_v{version}.pkl\"\n",
    "    while os.path.exists(f\"{file_path}{filename}\"):\n",
    "        version += 1\n",
    "        filename = f\"{base_filename}_v{version}.pkl\"\n",
    "    joblib.dump(models, f\"{file_path}{filename}\")\n",
    "    return filename\n",
    "\n",
    "# 업데이트된 모델 저장\n",
    "debug_message(\"업데이트된 모델 저장 중...\")\n",
    "saved_filename = save_model_with_versioning(models, file_path, \"updated_lightgbm_models\")\n",
    "debug_message(f\"업데이트된 모델 저장 완료: {saved_filename}\")\n",
    "\n",
    "print(f\"LightGBM models updated and saved as {saved_filename}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Not for Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 로드\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로딩\n",
    "data = pd.read_csv(f'{file_path}survey_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 데이터를 원-핫 인코딩\n",
    "categorical_features = ['gender', 'mbti', 'job']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# 전처리 파이프라인 설정\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# 타겟 변수와 피처 분리\n",
    "X = data[['age', 'gender', 'mbti', 'job']]\n",
    "y = data[['work', 'edu', 'free_time', 'health', 'chores']]\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 전처리\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# 각 타겟에 대한 모델 훈련 및 예측\n",
    "mse = []\n",
    "predictions = []\n",
    "for target in y.columns:\n",
    "    model = LGBMRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train[target])\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse.append(mean_squared_error(y_test[target], y_pred))\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "# 예측 결과를 데이터프레임으로 변환\n",
    "y_pred_df = pd.DataFrame(zip(*predictions), columns=y.columns)\n",
    "\n",
    "# MSE 결과 출력\n",
    "for idx, target in enumerate(y.columns):\n",
    "    print(f'MSE for {target}: {format(mse[idx], \".4f\")}')\n",
    "\n",
    "# 전체 MSE 계산 및 출력\n",
    "overall_mse = mean_squared_error(y_test, y_pred_df)\n",
    "print(\"Overall MSE:\", format(overall_mse, \".4f\"))\n",
    "\n",
    "# 예측 결과 데이터프레임 출력, 지수 표현식 없이\n",
    "print(y_pred_df.to_string(index=False, header=True, float_format=\"{:0.4f}\".format))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost 빌드 (너무 메모리 많이 먹고 오래 걸려서 사용 안함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 초기 모델 빌드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = 'your/file/path/'\n",
    "\n",
    "# 1. 초기 모델 생성용 데이터 로드\n",
    "# 사용자 정보 데이터를 CSV 파일에서 불러옵니다.\n",
    "user_info = pd.read_csv(f'{file_path}user_info.csv')\n",
    "\n",
    "# 초기 y 값 데이터를 CSV 파일에서 불러옵니다.\n",
    "schedule_1 = pd.read_csv(f'{file_path}schedule_1.csv')\n",
    "\n",
    "# 2. 초기 y 값 설정\n",
    "# 초기 y 값으로 schedule_1 데이터를 사용합니다.\n",
    "y_df = schedule_1.copy()\n",
    "\n",
    "# 3. 데이터 분할\n",
    "# X 변수는 사용자 정보의 특정 열들을 선택합니다.\n",
    "X = user_info[['age', 'gender', 'mbti', 'job']]\n",
    "# 데이터를 학습용과 테스트용으로 분할합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. 원-핫 인코딩 및 전처리 파이프라인 설정\n",
    "# 범주형 변수를 원-핫 인코딩합니다.\n",
    "categorical_features = ['gender', 'mbti', 'job']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# ColumnTransformer를 사용해 범주형 변수 전처리 및 나머지 변수는 그대로 유지합니다.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# 5. 전처리\n",
    "# 학습 데이터에 대해 전처리를 수행합니다.\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# 6. 각 타겟에 대한 모델 훈련 및 저장\n",
    "# 하이퍼파라미터 그리드를 설정합니다.\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # 부스팅 반복 횟수\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # 학습률\n",
    "    'max_depth': [3, 5, 7],  # 트리의 최대 깊이\n",
    "    'min_child_weight': [1, 3, 5],  # 자식 노드를 가지기 위한 최소 가중치 합\n",
    "    'subsample': [0.6, 0.8, 1.0],  # 각 트리마다 사용되는 데이터 샘플 비율\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]  # 각 트리마다 사용되는 피처 샘플 비율\n",
    "}\n",
    "\n",
    "# 각 타겟에 대해 모델을 훈련하고 최적의 하이퍼파라미터를 찾습니다.\n",
    "models = {}\n",
    "for target in y_df.columns:\n",
    "    model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train[target])\n",
    "    models[target] = grid_search.best_estimator_\n",
    "\n",
    "# 7. 모델 저장\n",
    "joblib.dump(models, f'{file_path}initial_xgboost_models.pkl')\n",
    "joblib.dump(preprocessor, f'{file_path}preprocessor.pkl')\n",
    "\n",
    "print(\"Initial XGBoost models created and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = 'your/file/path/'\n",
    "\n",
    "# 1. 기존 모델 로드\n",
    "# 초기 모델과 전처리기를 파일에서 로드합니다.\n",
    "models = joblib.load(f'{file_path}initial_xgboost_models.pkl')\n",
    "preprocessor = joblib.load(f'{file_path}preprocessor.pkl')\n",
    "\n",
    "# 2. schedule_2에서 업데이트 데이터 로드\n",
    "# schedule_2에서 최신 데이터를 불러옵니다.\n",
    "schedule_2 = pd.read_csv(f'{file_path}schedule_2.csv')\n",
    "\n",
    "# 3. user_info에서 사용자 정보 데이터 로드\n",
    "# 사용자 정보 데이터를 CSV 파일에서 불러옵니다.\n",
    "user_info = pd.read_csv(f'{file_path}user_info.csv')\n",
    "\n",
    "# 4. 업데이트할 X, y 설정\n",
    "# X 업데이트 데이터를 설정합니다.\n",
    "X_update = user_info[['age', 'gender', 'mbti', 'job']]\n",
    "# y 업데이트 데이터를 설정합니다.\n",
    "y_update = schedule_2.iloc[-1].values.reshape(1, -1)\n",
    "\n",
    "# 5. 전처리\n",
    "# 업데이트 데이터에 대해 전처리를 수행합니다.\n",
    "X_update = preprocessor.transform(X_update)\n",
    "\n",
    "# 6. 모델 업데이트\n",
    "for idx, target in enumerate(schedule_2.columns):\n",
    "    models[target].fit(X_update, y_update[:, idx])\n",
    "\n",
    "# 7. 업데이트된 모델 저장\n",
    "joblib.dump(models, f'{file_path}updated_xgboost_models.pkl')\n",
    "\n",
    "print(\"XGBoost models updated and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Not for Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 로드\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로딩\n",
    "data = pd.read_csv(f'{file_path}survey_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 데이터를 원-핫 인코딩\n",
    "categorical_features = ['gender', 'mbti', 'job']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# 전처리 파이프라인 설정\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# 타겟 변수와 피처 분리\n",
    "X = data[['age', 'gender', 'mbti', 'job']]\n",
    "y = data[['work', 'edu', 'free_time', 'health', 'chores']]\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 전처리 및 변환\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# 각 타겟에 대한 모델 훈련 및 예측 결과 저장\n",
    "predictions = []\n",
    "for i, target in enumerate(y.columns):\n",
    "    model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train.iloc[:, i])\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions.append(y_pred)\n",
    "    print(f'MSE for {target}: {mean_squared_error(y_test.iloc[:, i], y_pred)}')\n",
    "\n",
    "# 예측 결과를 데이터프레임으로 변환\n",
    "y_pred_df = pd.DataFrame(predictions).T  # Transpose to align with y_test's shape\n",
    "y_pred_df.columns = y_test.columns\n",
    "\n",
    "# 예측 데이터프레임과 실제 데이터프레임의 MSE 계산\n",
    "mse = mean_squared_error(y_test, y_pred_df)\n",
    "print(\"Overall MSE:\", format(mse, \".4f\"))\n",
    "\n",
    "# 예측 결과 데이터프레임 출력, 지수 표현식 없이\n",
    "print(y_pred_df.to_string(index=False, header=True, float_format=\"{:0.4f}\".format))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝 모델 (시간 없어서 사용 안함 (데이터도 없어))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow 빌드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로딩\n",
    "data = pd.read_csv(f'{file_path}survey_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 데이터를 원-핫 인코딩\n",
    "categorical_features = ['gender', 'mbti', 'job']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# 전처리 파이프라인 설정\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# 타겟 변수와 피처 분리\n",
    "X = data[['age', 'gender', 'mbti', 'job']]\n",
    "y = data[['work', 'edu', 'free_time', 'health', 'chores']]\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 전처리 및 변환\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 모델 구성\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_dim=X_train.shape[1]),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(5)  # 출력 레이어: 5개 타겟 변수\n",
    "])\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 및 평가\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# MSE 계산\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "mse_value = mse(y_test, y_pred).numpy()\n",
    "print(\"MSE:\", format(mse_value, \".4f\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시간 슬라이싱\n",
    "\n",
    "- 딥러닝 모델로 목표에 대한 업무를 중요도와 긴급도를 기준으로 시간 슬라이싱\n",
    "- 중요도(weight), 긴급도(fire_data, complexity)를 통해 가용 시간 하루 시간 단위로 슬라이싱\n",
    "- 슬라이싱 된 작은 목표 달성시 계속, 미 달성시(부분 미달성, 전체 미달성) 슬라이싱에 다시 반영"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow 빌드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 작업 중요도 예측 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "\n",
    "# 디버깅 메시지 출력 함수\n",
    "def debug_message(message):\n",
    "    print(f\"[디버깅] {message}\")\n",
    "\n",
    "# 모델 버져닝 함수\n",
    "def save_model_with_versioning(model, file_path, base_filename):\n",
    "    version = 1\n",
    "    filename = f\"{base_filename}_v{version}.h5\"\n",
    "    while os.path.exists(os.path.join(file_path, filename)):\n",
    "        version += 1\n",
    "        filename = f\"{base_filename}_v{version}.h5\"\n",
    "    model.save(os.path.join(file_path, filename))\n",
    "    return filename\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "file_path = './refer/output/'\n",
    "debug_message(\"작업 중요도 데이터 로드 중...\")\n",
    "data = pd.read_csv(f'{file_path}data.csv')\n",
    "debug_message(\"작업 중요도 데이터 로드 완료\")\n",
    "\n",
    "# 범주형 데이터에 대한 원-핫 인코딩\n",
    "debug_message(\"범주형 데이터 원-핫 인코딩 중...\")\n",
    "encoder = OneHotEncoder()\n",
    "encoded_data = encoder.fit_transform(data[['gender', 'job', 'mbti']]).toarray()\n",
    "debug_message(\"범주형 데이터 원-핫 인코딩 완료\")\n",
    "\n",
    "# 인코딩된 데이터와 수치형 데이터를 결합\n",
    "numeric_data = data[['age', 'work', 'edu', 'free_time', 'health', 'chores']].values\n",
    "X = np.hstack((encoded_data, numeric_data))\n",
    "\n",
    "# 데이터 정규화\n",
    "debug_message(\"데이터 정규화 중...\")\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "debug_message(\"데이터 정규화 완료\")\n",
    "\n",
    "# 목표 변수 정의 (중요도)\n",
    "y = data[['work', 'edu', 'free_time', 'health', 'chores']].values\n",
    "\n",
    "# 데이터를 학습용과 테스트용으로 분할\n",
    "debug_message(\"데이터 학습용 및 테스트용 분할 중...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "debug_message(\"데이터 학습용 및 테스트용 분할 완료\")\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "learning_rate = 0.001  # 학습률 결정: 학습 과정에서 가중치가 조정되는 속도\n",
    "batch_size = 32        # 배치 크기: 한 번의 훈련 반복에서 사용되는 샘플의 수\n",
    "epochs = 100           # 에포크 수: 전체 데이터셋을 훈련하는 반복 횟수\n",
    "hidden_layer1_size = 128  # 첫 번째 은닉층의 노드 수 결정: 모델의 복잡성 조절\n",
    "hidden_layer2_size = 64   # 두 번째 은닉층의 노드 수 결정: 모델의 복잡성 조절\n",
    "\n",
    "# TensorFlow/Keras를 사용하여 신경망 모델 정의\n",
    "debug_message(\"신경망 모델 정의 중...\")\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_layer1_size, input_dim=X_train.shape[1], activation='relu'))  # ReLU 활성화 함수 사용: sigmoid, tanh, leakyrelu 얘기 해야지\n",
    "model.add(Dense(hidden_layer2_size, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1]))\n",
    "debug_message(\"신경망 모델 정의 완료\")\n",
    "\n",
    "# 모델 컴파일\n",
    "debug_message(\"모델 컴파일 중...\")\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')  # Adam 옵티마이저 사용: NAG 사용하는 NAdam, Momentum 사용하는 Adam 중 Adam 사용\n",
    "debug_message(\"모델 컴파일 완료\")\n",
    "\n",
    "# 모델 학습\n",
    "debug_message(\"모델 학습 중...\")\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "debug_message(\"모델 학습 완료\")\n",
    "\n",
    "# 모델 평가\n",
    "debug_message(\"모델 평가 중...\")\n",
    "train_loss = model.evaluate(X_train, y_train)\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(f'훈련 손실: {train_loss:.4f}')\n",
    "print(f'테스트 손실: {test_loss:.4f}')\n",
    "\n",
    "# 예측 및 평가 (평균 제곱 오차 사용)\n",
    "debug_message(\"예측 및 평가 중...\")\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "print(f'훈련 MSE: {train_mse:.4f}')\n",
    "print(f'테스트 MSE: {test_mse:.4f}')\n",
    "debug_message(\"예측 및 평가 완료\")\n",
    "\n",
    "# 모델 저장\n",
    "debug_message(\"모델 저장 중...\")\n",
    "saved_filename = save_model_with_versioning(model, file_path, \"importance_model\")\n",
    "debug_message(f\"모델 저장 완료: {saved_filename}\")\n",
    "\n",
    "print(f\"Initial importance model created and saved as {saved_filename}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 작업 긴급도 예측 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "\n",
    "# 디버깅 메시지 출력 함수\n",
    "def debug_message(message):\n",
    "    print(f\"[디버깅] {message}\")\n",
    "\n",
    "# 모델 저장 함수\n",
    "def save_model_with_versioning(model, file_path, base_filename):\n",
    "    version = 1\n",
    "    filename = f\"{base_filename}_v{version}.h5\"\n",
    "    while os.path.exists(os.path.join(file_path, filename)):\n",
    "        version += 1\n",
    "        filename = f\"{base_filename}_v{version}.h5\"\n",
    "    model.save(os.path.join(file_path, filename))\n",
    "    return filename\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "file_path = './refer/output/'\n",
    "debug_message(\"작업 긴급도 데이터 로드 중...\")\n",
    "data = pd.read_csv(f'{file_path}data.csv')\n",
    "debug_message(\"작업 긴급도 데이터 로드 완료\")\n",
    "\n",
    "# 초기 긴급도 기준 생성\n",
    "debug_message(\"긴급도 관련 데이터 생성 중...\")\n",
    "data['start_date'] = pd.to_datetime(data['start_date'])\n",
    "data['end_date'] = pd.to_datetime(data['end_date'])\n",
    "data['days_left'] = (data['end_date'] - data['start_date']).dt.days\n",
    "data['urgency'] = 1 / (data['days_left'] + 1)  # 마감일이 가까울수록 긴급도가 높아짐\n",
    "debug_message(\"긴급도 관련 데이터 생성 완료\")\n",
    "\n",
    "# 긴급도 모델을 위한 입력 데이터 준비\n",
    "X_urgency = data[['start_date', 'end_date', 'complexity']].copy()\n",
    "X_urgency['start_date'] = (X_urgency['start_date'] - X_urgency['start_date'].min()).dt.days\n",
    "X_urgency['end_date'] = (X_urgency['end_date'] - X_urgency['end_date'].min()).dt.days\n",
    "y_urgency = data['urgency'].values\n",
    "\n",
    "# 데이터 정규화\n",
    "debug_message(\"데이터 정규화 중...\")\n",
    "scaler = StandardScaler()\n",
    "X_urgency = scaler.fit_transform(X_urgency)\n",
    "debug_message(\"데이터 정규화 완료\")\n",
    "\n",
    "# 데이터를 학습용과 테스트용으로 분할\n",
    "debug_message(\"데이터 학습용 및 테스트용 분할 중...\")\n",
    "X_train_urgency, X_test_urgency, y_train_urgency, y_test_urgency = train_test_split(X_urgency, y_urgency, test_size=0.2, random_state=42)\n",
    "debug_message(\"데이터 학습용 및 테스트용 분할 완료\")\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "learning_rate = 0.001  # 학습률 결정: 학습 과정에서 가중치가 조정되는 속도\n",
    "batch_size = 32        # 배치 크기 결정: 한 번의 훈련 반복에서 사용되는 샘플의 수\n",
    "epochs = 100           # 에포크 수 결정: 전체 데이터셋을 훈련하는 반복 횟수\n",
    "hidden_layer1_size = 128  # 첫 번째 은닉층의 노드 수 결정: 모델의 복잡성 조절\n",
    "hidden_layer2_size = 64   # 두 번째 은닉층의 노드 수 결정: 모델의 복잡성 조절\n",
    "\n",
    "# TensorFlow/Keras를 사용하여 신경망 모델 정의\n",
    "debug_message(\"신경망 모델 정의 중...\")\n",
    "urgency_model = Sequential()\n",
    "urgency_model.add(Dense(hidden_layer1_size, input_dim=X_train_urgency.shape[1], activation='relu'))  # ReLU 활성화 함수 사용: 비선형성을 추가하여 모델의 학습 능력 향상\n",
    "urgency_model.add(Dense(hidden_layer2_size, activation='relu'))\n",
    "urgency_model.add(Dense(1))\n",
    "debug_message(\"신경망 모델 정의 완료\")\n",
    "\n",
    "# 모델 컴파일\n",
    "debug_message(\"모델 컴파일 중...\")\n",
    "urgency_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')  # Adam 옵티마이저 사용: NAdam도 \n",
    "debug_message(\"모델 컴파일 완료\")\n",
    "\n",
    "# 모델 학습\n",
    "debug_message(\"모델 학습 중...\")\n",
    "urgency_model.fit(X_train_urgency, y_train_urgency, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "debug_message(\"모델 학습 완료\")\n",
    "\n",
    "# 모델 평가\n",
    "debug_message(\"모델 평가 중...\")\n",
    "train_loss_urgency = urgency_model.evaluate(X_train_urgency, y_train_urgency)\n",
    "test_loss_urgency = urgency_model.evaluate(X_test_urgency, y_test_urgency)\n",
    "print(f'훈련 긴급도 손실: {train_loss_urgency:.4f}')\n",
    "print(f'테스트 긴급도 손실: {test_loss_urgency:.4f}')\n",
    "\n",
    "# 예측 및 평가 (평균 제곱 오차 사용)\n",
    "debug_message(\"예측 및 평가 중...\")\n",
    "y_pred_train_urgency = urgency_model.predict(X_train_urgency)\n",
    "y_pred_test_urgency = urgency_model.predict(X_test_urgency)\n",
    "train_mse_urgency = mean_squared_error(y_train_urgency, y_pred_train_urgency)\n",
    "test_mse_urgency = mean_squared_error(y_test_urgency, y_pred_test_urgency)\n",
    "print(f'훈련 긴급도 MSE: {train_mse_urgency:.4f}')\n",
    "print(f'테스트 긴급도 MSE: {test_mse_urgency:.4f}')\n",
    "debug_message(\"예측 및 평가 완료\")\n",
    "\n",
    "# 모델 저장\n",
    "debug_message(\"모델 저장 중...\")\n",
    "saved_filename = save_model_with_versioning(urgency_model, file_path, \"urgency_model\")\n",
    "debug_message(f\"모델 저장 완료: {saved_filename}\")\n",
    "\n",
    "print(f\"Initial urgency model created and saved as {saved_filename}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 시간 분배 및 재분배 계산 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# 디버깅 메시지 출력 함수\n",
    "def debug_message(message):\n",
    "    print(f\"[디버깅] {message}\")\n",
    "\n",
    "# 모델 로드 함수\n",
    "def load_latest_model(file_path, base_filename):\n",
    "    version = 1\n",
    "    filename = f\"{base_filename}_v{version}.h5\"\n",
    "    while os.path.exists(os.path.join(file_path, filename)):\n",
    "        version += 1\n",
    "        filename = f\"{base_filename}_v{version}.h5\"\n",
    "    if version == 1:\n",
    "        raise FileNotFoundError(f\"No model found for {base_filename}\")\n",
    "    return load_model(os.path.join(file_path, filename))\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = './refer/output/'\n",
    "debug_message(\"사용자 정보 데이터를 불러오는 중...\")\n",
    "user_info = pd.read_csv(f'{file_path}random_user_info.csv')\n",
    "debug_message(\"사용자 정보 데이터 로드 완료\")\n",
    "\n",
    "debug_message(\"schedule_2 데이터를 불러오는 중...\")\n",
    "schedule_2 = pd.read_csv(f'{file_path}random_schedule_2.csv')\n",
    "debug_message(\"schedule_2 데이터 로드 완료\")\n",
    "\n",
    "debug_message(\"user_check 데이터를 불러오는 중...\")\n",
    "user_check = pd.read_csv(f'{file_path}user_check.csv') # 유저의 작업 달성 정도 or 여부를 받아오기\n",
    "debug_message(\"user_check 데이터 로드 완료\")\n",
    "\n",
    "debug_message(\"user_time 데이터를 불러오는 중...\")\n",
    "user_time = pd.read_csv(f'{file_path}user_time.csv') # 유저의 하루 가용 시간을 받아오기\n",
    "debug_message(\"user_time 데이터 로드 완료\")\n",
    "\n",
    "# 오늘 날짜의 daily_time 설정\n",
    "today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "daily_time_row = user_time[user_time['date'] == today_date]\n",
    "\n",
    "if not daily_time_row.empty:\n",
    "    daily_time = daily_time_row['daily_time'].values[0]\n",
    "else:\n",
    "    raise ValueError(f\"No daily time data available for today ({today_date})\")\n",
    "\n",
    "# 중요도 모델 및 긴급도 모델 로드\n",
    "debug_message(\"중요도 모델 로드 중...\")\n",
    "importance_model = load_latest_model(file_path, \"importance_model\")\n",
    "debug_message(\"중요도 모델 로드 완료\")\n",
    "\n",
    "debug_message(\"긴급도 모델 로드 중...\")\n",
    "urgency_model = load_latest_model(file_path, \"urgency_model\")\n",
    "debug_message(\"긴급도 모델 로드 완료\")\n",
    "\n",
    "# 범주형 데이터에 대한 원-핫 인코딩\n",
    "debug_message(\"범주형 데이터에 대한 원-핫 인코딩 중...\")\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded_data = encoder.fit_transform(user_info[['gender', 'job', 'mbti']]).toarray()\n",
    "debug_message(\"범주형 데이터에 대한 원-핫 인코딩 완료\")\n",
    "\n",
    "# 인코딩된 데이터와 수치형 데이터를 결합\n",
    "numeric_data = user_info[['age']].values\n",
    "X_user_info = np.hstack((encoded_data, numeric_data))\n",
    "\n",
    "# 데이터 정규화\n",
    "debug_message(\"데이터 정규화 중...\")\n",
    "scaler = StandardScaler()\n",
    "X_user_info = scaler.fit_transform(X_user_info)\n",
    "debug_message(\"데이터 정규화 완료\")\n",
    "\n",
    "# 중요도 예측\n",
    "debug_message(\"중요도 예측 중...\")\n",
    "importance_scores = importance_model.predict(X_user_info)\n",
    "debug_message(\"중요도 예측 완료\")\n",
    "\n",
    "# 긴급도 예측\n",
    "debug_message(\"긴급도 예측 중...\")\n",
    "X_schedule = schedule_2[['start_date', 'end_date', 'complexity']].copy()\n",
    "X_schedule['start_date'] = pd.to_datetime(X_schedule['start_date'])\n",
    "X_schedule['end_date'] = pd.to_datetime(X_schedule['end_date'])\n",
    "X_schedule['start_date'] = (X_schedule['start_date'] - X_schedule['start_date'].min()).dt.days\n",
    "X_schedule['end_date'] = (X_schedule['end_date'] - X_schedule['end_date'].min()).dt.days\n",
    "X_schedule = scaler.fit_transform(X_schedule)\n",
    "urgency_scores = urgency_model.predict(X_schedule)\n",
    "debug_message(\"긴급도 예측 완료\")\n",
    "\n",
    "# 중요도와 긴급도를 기반으로 작업 분배\n",
    "debug_message(\"작업 분배 중...\")\n",
    "total_importance = np.sum(importance_scores)\n",
    "total_urgency = np.sum(urgency_scores)\n",
    "weights = (importance_scores / total_importance) + (urgency_scores / total_urgency)\n",
    "weights /= np.sum(weights)  # 가중치의 합이 1이 되도록 정규화\n",
    "\n",
    "# 작업 시간 분배\n",
    "work_distribution = daily_time * weights.flatten()\n",
    "debug_message(f\"작업 시간 분배 완료: {work_distribution}\")\n",
    "\n",
    "# 유저의 작업 달성 정도를 반영한 재분배\n",
    "debug_message(\"유저의 작업 달성 정도 반영한 재분배 중...\")\n",
    "achievement_ratios = user_check['achievement_ratio'].values\n",
    "adjusted_distribution = work_distribution * achievement_ratios\n",
    "adjusted_distribution /= np.sum(adjusted_distribution)  # 조정된 분배 시간의 합이 daily_time이 되도록 정규화\n",
    "adjusted_work_distribution = daily_time * adjusted_distribution\n",
    "debug_message(f\"조정된 작업 시간 분배 완료: {adjusted_work_distribution}\")\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"초기 작업 시간 분배: {work_distribution}\")\n",
    "print(f\"조정된 작업 시간 분배: {adjusted_work_distribution}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
