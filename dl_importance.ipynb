{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-06-27 17:51:06</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:05.93        </td></tr>\n",
       "<tr><td>Memory:      </td><td>12.4/15.6 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: nan<br>Logical resource usage: 4.0/16 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status    </th><th>loc            </th><th>batch_norm  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  epochs</th><th style=\"text-align: right;\">  hidden_layer1_size</th><th style=\"text-align: right;\">  hidden_layer2_size</th><th style=\"text-align: right;\">  l1_lambda</th><th style=\"text-align: right;\">  l2_lambda</th><th style=\"text-align: right;\">  lr</th><th>optimizer  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  mean_squared_error</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_6072e_00000</td><td>TERMINATED</td><td>127.0.0.1:33356</td><td>False       </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">           0.3</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">                 256</td><td style=\"text-align: right;\">                 128</td><td style=\"text-align: right;\">     0.0001</td><td style=\"text-align: right;\">     0.0001</td><td style=\"text-align: right;\">0.01</td><td>adam       </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       0.0625463</td><td style=\"text-align: right;\">                 inf</td><td style=\"text-align: right;\">         inf</td><td style=\"text-align: right;\">       inf</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "2024-06-27 17:51:06,069\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'C:/Users/USER/ray_results/train_model_2024-06-27_17-51-00' in 0.0050s.\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\function_base.py:4655: RuntimeWarning: invalid value encountered in subtract\n",
      "  diff_b_a = subtract(b, a)\n",
      "2024-06-27 17:51:06,087\tINFO tune.py:1041 -- Total run time: 6.03 seconds (5.92 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[디버깅] Ray Tune 하이퍼파라미터 최적화 완료\n",
      "Best config:  {'lr': 0.01, 'batch_size': 16, 'epochs': 10, 'hidden_layer1_size': 256, 'hidden_layer2_size': 128, 'dropout_rate': 0.3, 'batch_norm': False, 'optimizer': 'adam', 'l2_lambda': 0.0001, 'l1_lambda': 0.0001, 'num_cpus': 4, 'num_gpus': 0}\n",
      "[디버깅] 최적의 하이퍼파라미터 찾기 완료\n",
      "[디버깅] 최적의 하이퍼파라미터로 모델 재학습 중...\n",
      "[디버깅] 모델 생성 중...\n",
      "[디버깅] 모델 생성 완료\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4670/4670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 1.9018 - mse: 1.8138 - val_loss: 1.5450 - val_mse: 1.5430\n",
      "Epoch 2/10\n",
      "\u001b[1m4670/4670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 1.5494 - mse: 1.5469 - val_loss: 1.5457 - val_mse: 1.5415\n",
      "Epoch 3/10\n",
      "\u001b[1m4670/4670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 1.5445 - mse: 1.5400 - val_loss: 1.5464 - val_mse: 1.5416\n",
      "Epoch 4/10\n",
      "\u001b[1m4670/4670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 1.5445 - mse: 1.5397 - val_loss: 1.5457 - val_mse: 1.5409\n",
      "Epoch 5/10\n",
      "\u001b[1m4670/4670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 1.5474 - mse: 1.5426 - val_loss: 1.5455 - val_mse: 1.5407\n",
      "Epoch 6/10\n",
      "\u001b[1m4670/4670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 1.5434 - mse: 1.5385 - val_loss: 1.5459 - val_mse: 1.5411\n",
      "Epoch 7/10\n",
      "\u001b[1m4670/4670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 1.5450 - mse: 1.5401 - val_loss: 1.5464 - val_mse: 1.5416\n",
      "Epoch 8/10\n",
      "\u001b[1m4670/4670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 1.5436 - mse: 1.5388 - val_loss: 1.5470 - val_mse: 1.5421\n",
      "Epoch 9/10\n",
      "\u001b[1m4670/4670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 1.5451 - mse: 1.5403 - val_loss: 1.5455 - val_mse: 1.5406\n",
      "Epoch 10/10\n",
      "\u001b[1m4670/4670\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 1.5432 - mse: 1.5384 - val_loss: 1.5490 - val_mse: 1.5442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 17:52:00,911\tERROR experiment_analysis.py:467 -- No checkpoints have been found for trial train_model_6072e_00000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[디버깅] 모델 재학습 완료\n",
      "[디버깅] 최종 평가 결과 분석 중...\n",
      "[디버깅] 모델 생성 중...\n",
      "[디버깅] 모델 생성 완료\n",
      "[디버깅] 최종 모델 평가 중...\n",
      "\u001b[1m2919/2919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 578us/step - loss: 15.8259 - mse: 15.5553\n",
      "\u001b[1m730/730\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 570us/step - loss: 15.8046 - mse: 15.5340\n",
      "\u001b[1m2919/2919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 483us/step\n",
      "\u001b[1m730/730\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 428us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 MSE: 15.5548\n",
      "테스트 MSE: 15.5446\n",
      "[디버깅] 최종 모델 평가 완료\n",
      "[디버깅] 모델 저장 중...\n",
      "[디버깅] 모델 저장 완료: importance_model_v1.h5\n",
      "Initial importance model created and saved as importance_model_v1.h5.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.integration.keras import TuneReportCallback\n",
    "from ray.air import session\n",
    "import os\n",
    "import multiprocessing\n",
    "from refer.module.func import debug_message, save_model_with_versioning, trial_dirname_creator\n",
    "import random\n",
    "\n",
    "# 시스템 정보 가져오기\n",
    "num_cpus = multiprocessing.cpu_count()\n",
    "num_gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Ray 초기화\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# 데이터 로드 및 전처리 (DB에서 데이터 가져오기로 수정필요)\n",
    "file_path = './refer/output/'\n",
    "debug_message(\"작업 중요도 데이터 로드 중...\")\n",
    "data = pd.read_csv(f'{file_path}user_info.csv')\n",
    "debug_message(\"작업 중요도 데이터 로드 완료\")\n",
    "\n",
    "# 목표 변수 (중요도) 데이터 로드\n",
    "debug_message(\"작업 중요도 목표 변수 로드 중...\")\n",
    "target_data = pd.read_csv(f'{file_path}user_weight.csv')\n",
    "debug_message(\"작업 중요도 목표 변수 로드 완료\")\n",
    "\n",
    "# 범주형 데이터에 대한 원-핫 인코딩\n",
    "debug_message(\"범주형 데이터 원-핫 인코딩 중...\")\n",
    "encoder = OneHotEncoder()\n",
    "encoded_data = encoder.fit_transform(data[['gender', 'job', 'mbti']]).toarray()\n",
    "debug_message(\"범주형 데이터 원-핫 인코딩 완료\")\n",
    "\n",
    "# 인코딩된 데이터와 수치형 데이터를 결합\n",
    "numeric_data = data[['age']].values  # 나이 데이터는 별도로 결합\n",
    "X = np.hstack((encoded_data, numeric_data))\n",
    "\n",
    "# 데이터 정규화\n",
    "debug_message(\"데이터 정규화 중...\")\n",
    "scaler = StandardScaler()  # RobustScaler를 사용할 수도 있음\n",
    "X = scaler.fit_transform(X)\n",
    "debug_message(\"데이터 정규화 완료\")\n",
    "\n",
    "# 목표 변수 정의 (중요도)\n",
    "y = target_data[['work', 'edu', 'free_time', 'health', 'chores', 'category_else']].values\n",
    "\n",
    "# 데이터를 학습용과 테스트용으로 분할\n",
    "debug_message(\"데이터 학습용 및 테스트용 분할 중...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "debug_message(\"데이터 학습용 및 테스트용 분할 완료\")\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))\n",
    "\n",
    "# 모델 생성 함수\n",
    "def create_model(config):\n",
    "    debug_message(\"모델 생성 중...\")\n",
    "    model = Sequential()\n",
    "    model.add(Dense(config[\"hidden_layer1_size\"], input_dim=X_train.shape[1], activation='relu', \n",
    "                    kernel_regularizer=tf.keras.regularizers.l1_l2(l1=config[\"l1_lambda\"], l2=config[\"l2_lambda\"])))\n",
    "    if config[\"batch_norm\"]:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(config[\"dropout_rate\"]))\n",
    "    model.add(Dense(config[\"hidden_layer2_size\"], activation='relu',\n",
    "                    kernel_regularizer=tf.keras.regularizers.l1_l2(l1=config[\"l1_lambda\"], l2=config[\"l2_lambda\"])))\n",
    "    if config[\"batch_norm\"]:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(config[\"dropout_rate\"]))\n",
    "    model.add(Dense(y_train.shape[1]))\n",
    "\n",
    "    optimizer_name = config[\"optimizer\"]\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=config[\"lr\"])\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=config[\"lr\"])\n",
    "    elif optimizer_name == \"rmsprop\":\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=config[\"lr\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])  # MSE를 메트릭으로 추가\n",
    "    debug_message(\"모델 생성 완료\")\n",
    "    return model\n",
    "\n",
    "# 학습 함수\n",
    "def train_model(config, X_train, y_train, X_test, y_test):\n",
    "    print(\"train_model 함수에서 config는 \", config)\n",
    "    try:\n",
    "        debug_message(\"모델 학습 시작...\")\n",
    "        model = create_model(config)\n",
    "        log_dir = os.path.join(\"logs\", f\"trial_{tune.get_trial_id()[:8]}\")\n",
    "        os.makedirs(log_dir, exist_ok=True)  # 로그 디렉토리가 없으면 생성\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        tune_report_callback = TuneReportCallback({\"mean_squared_error\": \"val_mse\"})\n",
    "        history = model.fit(X_train, y_train, \n",
    "                            epochs=config[\"epochs\"], \n",
    "                            batch_size=config[\"batch_size\"], \n",
    "                            validation_split=0.2, \n",
    "                            verbose=1,\n",
    "                            callbacks=[tensorboard_callback, tune_report_callback])\n",
    "        debug_message(\"모델 학습 완료\")\n",
    "        \n",
    "        # 모델 평가\n",
    "        debug_message(\"모델 평가 중...\")\n",
    "        train_loss = model.evaluate(X_train, y_train, verbose=1)\n",
    "        val_loss = model.evaluate(X_test, y_test, verbose=1)\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "        val_mse = mean_squared_error(y_test, y_pred_test)\n",
    "        debug_message(\"모델 평가 완료\")\n",
    "        \n",
    "        # 평가 결과 보고\n",
    "        session.report({\"mean_squared_error\": val_mse, \"train_loss\": train_loss, \"val_loss\": val_loss, \"train_mse\": train_mse, \"val_mse\": val_mse})\n",
    "    except Exception as e:\n",
    "        debug_message(f\"학습 중 오류 발생: {str(e)}\")\n",
    "        session.report({\"mean_squared_error\": float(\"inf\"), \"train_loss\": float(\"inf\"), \"val_loss\": float(\"inf\"), \"train_mse\": float(\"inf\"), \"val_mse\": float(\"inf\")})\n",
    "\n",
    "# Ray Tune을 사용한 하이퍼파라미터 최적화\n",
    "def tune_model():\n",
    "    config = {\n",
    "        'lr': tune.choice([0.0001, 0.01]),  # 학습률 결정: 학습 과정에서 가중치가 조정되는 속도\n",
    "        'batch_size': tune.choice([16, 64]),  # 배치 크기: 한 번의 훈련 반복에서 사용되는 샘플의 수\n",
    "        'epochs': tune.choice([10, 20]),  # 에포크 수: 전체 데이터셋을 훈련하는 반복 횟수\n",
    "        'hidden_layer1_size': tune.choice([64, 256]),  # 첫 번째 은닉층의 노드 수 결정: 모델의 복잡성 조절\n",
    "        'hidden_layer2_size': tune.choice([32, 128]),  # 두 번째 은닉층의 노드 수 결정: 모델의 복잡성 조절\n",
    "        'dropout_rate': tune.choice([0.1, 0.3]),  # 드롭아웃 비율: 과적합을 방지하기 위해 일부 뉴런을 무작위로 제외\n",
    "        'batch_norm': tune.choice([True, False]),  # 배치 정규화 사용 여부: 학습을 안정화하고 가속화\n",
    "        'optimizer': tune.choice(['adam', 'sgd', 'rmsprop']),  # 옵티마이저: 학습 과정에서 가중치를 업데이트하는 방법 결정\n",
    "        'l2_lambda': tune.choice([0.0001, 0.01]),  # L2 정규화: 가중치의 크기를 제한하여 과적합 방지\n",
    "        'l1_lambda': tune.choice([0.0001, 0.01]),  # L1 정규화: 가중치의 크기를 제한하여 과적합 방지\n",
    "        'num_cpus': 4,  # CPU 수 설정\n",
    "        'num_gpus': 0   # GPU 수 설정\n",
    "    }\n",
    "    # config에서 최대 에포크 값 추출\n",
    "    max_epochs = max(config['epochs'].categories)\n",
    "\n",
    "    # ASHA 스케줄러 설정: Asynchronous Successive Halving Algorithm\n",
    "    scheduler = ASHAScheduler(\n",
    "        # metric=\"mean_squared_error\",  # 최적화할 메트릭\n",
    "        # mode=\"min\",  # 최소화할 것인지, 최대화할 것인지 설정\n",
    "        max_t=max_epochs,  # 각 실험에서 실행할 최대 시간 또는 최대 스텝 (여기서는 에포크 수)\n",
    "        grace_period=1,  # 각 실험을 종료하기 전에 최소한으로 실행할 시간 또는 스텝\n",
    "        reduction_factor=2  # 리소스를 절감하기 위해 각 실험을 종료할 때마다 감소시킬 비율\n",
    "    )\n",
    "    \n",
    "    # Ray Tune을 사용하여 하이퍼파라미터 최적화 수행\n",
    "    debug_message(\"Ray Tune 하이퍼파라미터 최적화 시작...\")\n",
    "    analysis = tune.run(\n",
    "        tune.with_parameters(train_model, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test),  # 학습 함수\n",
    "        resources_per_trial={\"cpu\": config['num_cpus'], \"gpu\": config['num_gpus']},  # 각 시도에서 사용할 리소스 설정\n",
    "        config=config,  # 하이퍼파라미터 설정\n",
    "        num_samples=1,  # 샘플링 횟수: 각 설정으로 몇 번의 실험을 실행할지\n",
    "        scheduler=scheduler,  # 스케줄러 설정\n",
    "        verbose=1,  # 학습 과정 출력 레벨: 0은 출력 없음, 1은 진행 상태 막대 표시, 2는 자세한 로그 출력\n",
    "        trial_dirname_creator=trial_dirname_creator,  # 디렉토리 이름 생성 함수\n",
    "        metric='mean_squared_error',\n",
    "        mode='min'\n",
    "    )\n",
    "    debug_message(\"Ray Tune 하이퍼파라미터 최적화 완료\")\n",
    "    \n",
    "    # 최적의 하이퍼파라미터 출력\n",
    "    print(\"Best config: \", analysis.get_best_config(metric=\"mean_squared_error\", mode=\"min\"))\n",
    "    return analysis.get_best_config(metric=\"mean_squared_error\", mode=\"min\"), analysis\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 최적의 하이퍼파라미터 찾기\n",
    "    debug_message(\"최적의 하이퍼파라미터 찾기 시작...\")\n",
    "    best_config, analysis = tune_model()\n",
    "    debug_message(\"최적의 하이퍼파라미터 찾기 완료\")\n",
    "    \n",
    "    debug_message(\"최적의 하이퍼파라미터로 모델 재학습 중...\")\n",
    "    best_model = create_model(best_config)\n",
    "    best_model.fit(X_train, y_train, epochs=best_config['epochs'], batch_size=best_config['batch_size'], validation_split=0.2, verbose=1)\n",
    "    debug_message(\"모델 재학습 완료\")\n",
    "    \n",
    "    # 최종 평가 결과\n",
    "    debug_message(\"최종 평가 결과 분석 중...\")\n",
    "    best_trial = analysis.get_best_trial(\"mean_squared_error\", mode=\"min\", scope=\"all\")\n",
    "    best_trained_model = create_model(best_trial.config)\n",
    "    best_checkpoint_dir = analysis.get_best_checkpoint(best_trial)\n",
    "\n",
    "    if best_checkpoint_dir:\n",
    "        model_path = os.path.join(best_checkpoint_dir, \"checkpoint\")\n",
    "        best_trained_model.load_weights(model_path)\n",
    "\n",
    "    debug_message(\"최종 모델 평가 중...\")\n",
    "    train_loss = best_trained_model.evaluate(X_train, y_train, verbose=1)\n",
    "    test_loss = best_trained_model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "    y_pred_train = best_trained_model.predict(X_train)\n",
    "    y_pred_test = best_trained_model.predict(X_test)\n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "    \n",
    "    log_dir = os.path.join(\"logs\", \"final_model\")\n",
    "    os.makedirs(log_dir, exist_ok=True)  # 로그 디렉토리가 없으면 생성\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    print(f'훈련 MSE: {train_mse:.4f}')\n",
    "    print(f'테스트 MSE: {test_mse:.4f}')\n",
    "    debug_message(\"최종 모델 평가 완료\")\n",
    "    \n",
    "    # 모델 저장\n",
    "    debug_message(\"모델 저장 중...\")\n",
    "    saved_filename = save_model_with_versioning(best_trained_model, file_path, \"importance_model\")\n",
    "    debug_message(f\"모델 저장 완료: {saved_filename}\")\n",
    "    \n",
    "    print(f\"Initial importance model created and saved as {saved_filename}.\")\n",
    "\n",
    "    # Ray 종료\n",
    "    ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
