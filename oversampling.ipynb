{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설문 조사 데이터를 오버샘플링 하기 위한 코드\n",
    "- 샘플링 수를 늘려보자 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 필요한 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 랜덤 오버 샘플링을 위한 라이브러리\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파일 위치 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './refer/output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 설문 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def survey_processing(file_path = f'{file_path}survey.csv', output_path = f'{file_path}survey_preprocessed.csv'):\n",
    "    # CSV 파일을 불러옵니다.\n",
    "    survey_df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 우선순위에 대한 가중치 매핑을 정의합니다.\n",
    "    priority_weights = {\n",
    "        \"건강\": \"health\",\n",
    "        \"여가시간\": \"free_time\",\n",
    "        \"학업 및 자기계발\": \"edu\",\n",
    "        \"업무\": \"work\",\n",
    "        \"집안일\": \"chores\",\n",
    "        None: \"category_else\"\n",
    "    }\n",
    "    \n",
    "    # 각 우선순위 컬럼에 대해 가중치를 적용합니다.\n",
    "    for i in range(1, 6):\n",
    "        column_name = f\"본인이 생각하는 일과 별 중요도를 우선순위를 정하여 체크해주세요. (각 순위 별로 하나씩만 체크해주세요) [{i} 순위]\"\n",
    "        survey_df[column_name] = survey_df[column_name].map(priority_weights)\n",
    "    \n",
    "    # 필요한 컬럼들만 선택하고 리네임합니다.\n",
    "    columns_to_keep = [\n",
    "        \"설문자의 나이(만)는 어떻게 되십니까?\",\n",
    "        \"설문자의 해당사항을 체크해주세요.\",\n",
    "        \"설문자의 MBTI는 무엇입니까?\",\n",
    "        \"설문자의 성별은 어떻게 되십니까?\",\n",
    "        \"본인이 생각하는 일과 별 중요도를 우선순위를 정하여 체크해주세요. (각 순위 별로 하나씩만 체크해주세요) [1 순위]\",\n",
    "        \"본인이 생각하는 일과 별 중요도를 우선순위를 정하여 체크해주세요. (각 순위 별로 하나씩만 체크해주세요) [2 순위]\",\n",
    "        \"본인이 생각하는 일과 별 중요도를 우선순위를 정하여 체크해주세요. (각 순위 별로 하나씩만 체크해주세요) [3 순위]\",\n",
    "        \"본인이 생각하는 일과 별 중요도를 우선순위를 정하여 체크해주세요. (각 순위 별로 하나씩만 체크해주세요) [4 순위]\",\n",
    "        \"본인이 생각하는 일과 별 중요도를 우선순위를 정하여 체크해주세요. (각 순위 별로 하나씩만 체크해주세요) [5 순위]\"\n",
    "    ]\n",
    "    \n",
    "    survey_df = survey_df[columns_to_keep]\n",
    "    \n",
    "    # 컬럼명을 변환합니다.\n",
    "    column_mapping = {\n",
    "        \"설문자의 나이(만)는 어떻게 되십니까?\": \"age\",\n",
    "        \"설문자의 해당사항을 체크해주세요.\": \"job\",\n",
    "        \"설문자의 MBTI는 무엇입니까?\": \"mbti\",\n",
    "        \"설문자의 성별은 어떻게 되십니까?\": \"gender\",\n",
    "        \"본인이 생각하는 일과 별 중요도를 우선순위를 정하여 체크해주세요. (각 순위 별로 하나씩만 체크해주세요) [1 순위]\": \"priority_1\",\n",
    "        \"본인이 생각하는 일과 별 중요도를 우선순위를 정하여 체크해주세요. (각 순위 별로 하나씩만 체크해주세요) [2 순위]\": \"priority_2\",\n",
    "        \"본인이 생각하는 일과 별 중요도를 우선순위를 정하여 체크해주세요. (각 순위 별로 하나씩만 체크해주세요) [3 순위]\": \"priority_3\",\n",
    "        \"본인이 생각하는 일과 별 중요도를 우선순위를 정하여 체크해주세요. (각 순위 별로 하나씩만 체크해주세요) [4 순위]\": \"priority_4\",\n",
    "        \"본인이 생각하는 일과 별 중요도를 우선순위를 정하여 체크해주세요. (각 순위 별로 하나씩만 체크해주세요) [5 순위]\": \"priority_5\"\n",
    "    }\n",
    "    \n",
    "    survey_df.rename(columns=column_mapping, inplace=True)\n",
    "    \n",
    "    # 새로운 컬럼 'category_else'를 추가하고 모든 값을 0으로 설정합니다.\n",
    "    survey_df[\"category_else\"] = 0\n",
    "    \n",
    "    # 나이, 직업, MBTI 컬럼을 숫자 코드로 변환합니다.\n",
    "    age_mapping = {\n",
    "        \"0~14세\": (0, 14),\n",
    "        \"15~19세\": (15, 19),\n",
    "        \"20~24세\": (20, 24),\n",
    "        \"25~30세\": (25, 30),\n",
    "        \"31세 이상\": (31, 45)\n",
    "    }\n",
    "    job_mapping = {\n",
    "        \"초/중학생\": \"000\",\n",
    "        \"고등학생\": \"001\",\n",
    "        \"대학생 / 저학년 (1-2학년)\": \"002\",\n",
    "        \"대학생 / 고학년(3-4학년)\": \"003\",\n",
    "        \"구직자\": \"004\",\n",
    "        \"직장인\": \"005\",\n",
    "        \"자영업자\": \"006\",\n",
    "        \"프리랜서\": \"007\",\n",
    "        \"주부\": \"008\",\n",
    "        \"기타\": \"009\"\n",
    "    }\n",
    "    mbti_mapping = {\n",
    "        \"INTJ\": \"00\",\n",
    "        \"INTP\": \"01\",\n",
    "        \"ENTJ\": \"02\",\n",
    "        \"ENTP\": \"03\",\n",
    "        \"INFJ\": \"04\",\n",
    "        \"INFP\": \"05\",\n",
    "        \"ENFJ\": \"06\",\n",
    "        \"ENFP\": \"07\",\n",
    "        \"ISTJ\": \"08\",\n",
    "        \"ISFJ\": \"09\",\n",
    "        \"ESTJ\": \"10\",\n",
    "        \"ESFJ\": \"11\",\n",
    "        \"ISTP\": \"12\",\n",
    "        \"ISFP\": \"13\",\n",
    "        \"ESTP\": \"14\",\n",
    "        \"ESFP\": \"15\"\n",
    "    }\n",
    "    gender_mapping = {\n",
    "        \"남\": 0,\n",
    "        \"여\": 1\n",
    "    }\n",
    "\n",
    "    # 나이, 직업, MBTI, 성별 컬럼을 숫자 코드로 변환합니다.\n",
    "    survey_df['age'] = survey_df['age'].map(lambda x: np.random.randint(age_mapping[x][0], age_mapping[x][1] + 1))\n",
    "    survey_df['job'] = survey_df['job'].map(job_mapping)\n",
    "    survey_df['mbti'] = survey_df['mbti'].str.upper().map(mbti_mapping)\n",
    "    survey_df['gender'] = survey_df['gender'].map(gender_mapping)\n",
    "\n",
    "    # 우선순위별로 각 항목에 가중치를 적용한 값을 할당합니다.\n",
    "    work_col = []\n",
    "    edu_col = []\n",
    "    free_time_col = []\n",
    "    health_col = []\n",
    "    chores_col = []\n",
    "\n",
    "    for _, row in survey_df.iterrows():\n",
    "        work = 0\n",
    "        edu = 0\n",
    "        free_time = 0\n",
    "        health = 0\n",
    "        chores = 0\n",
    "\n",
    "        for i in range(1, 6):\n",
    "            if row[f'priority_{i}'] == 'work':\n",
    "                work = 1 - (i - 1) * 0.25\n",
    "            elif row[f'priority_{i}'] == 'edu':\n",
    "                edu = 1 - (i - 1) * 0.25\n",
    "            elif row[f'priority_{i}'] == 'free_time':\n",
    "                free_time = 1 - (i - 1) * 0.25\n",
    "            elif row[f'priority_{i}'] == 'health':\n",
    "                health = 1 - (i - 1) * 0.25\n",
    "            elif row[f'priority_{i}'] == 'chores':\n",
    "                chores = 1 - (i - 1) * 0.25\n",
    "        \n",
    "        work_col.append(work)\n",
    "        edu_col.append(edu)\n",
    "        free_time_col.append(free_time)\n",
    "        health_col.append(health)\n",
    "        chores_col.append(chores)\n",
    "    \n",
    "    survey_df['work'] = work_col\n",
    "    survey_df['edu'] = edu_col\n",
    "    survey_df['free_time'] = free_time_col\n",
    "    survey_df['health'] = health_col\n",
    "    survey_df['chores'] = chores_col\n",
    "    \n",
    "    # 사용하지 않는 우선순위 컬럼 삭제\n",
    "    survey_df.drop(columns=['priority_1', 'priority_2', 'priority_3', 'priority_4', 'priority_5'], inplace=True)\n",
    "    \n",
    "    # 컬럼 순서 재정렬\n",
    "    survey_df = survey_df[['age', 'job', 'mbti', 'gender', 'work', 'edu', 'free_time', 'health', 'chores', 'category_else']]\n",
    "    \n",
    "    # 수정된 데이터프레임을 새로운 CSV 파일로 저장합니다.\n",
    "    survey_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_processing()  # 함수를 호출하여 변환을 수행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 원본 데이터 단순 복제로 데이터 수를 10만개로 늘리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 복제 완료: 총 샘플 수 100000\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "data = pd.read_csv(f'{file_path}survey_preprocessed.csv')\n",
    "\n",
    "# 원하는 샘플 수\n",
    "target_samples = 100000\n",
    "replication_factor = target_samples // len(data)  # 필요한 복제 횟수 계산\n",
    "additional_samples = target_samples % len(data)   # 추가로 필요한 샘플 수\n",
    "\n",
    "# 데이터 복제\n",
    "oversampled_data = pd.concat([data] * replication_factor + [data.iloc[:additional_samples]])\n",
    "\n",
    "# 결과 저장\n",
    "oversampled_data.to_csv(f'{file_path}survey_replicated.csv', index=False)\n",
    "\n",
    "print(f\"데이터 복제 완료: 총 샘플 수 {len(oversampled_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 랜덤 오버 샘플링을 통해 전체 데이터의 수를 늘리기\n",
    "\n",
    "- 단, 랜덤 오버 샘플링은 이산형 데이터에 사용이 가능\n",
    "- 따라서 '0', '0.25', '0.5', '0.75', '1'을 범주화하여 제한을 우회\n",
    "- 이후 다시 범주형 데이터를 수치로 롤백하여 원본 데이터의 형식을 유지\n",
    "- 이 외의 숫자가 혹시 포함되어 있다면 디버깅을 위해 -1로 설정\n",
    "\n",
    "- 랜덤 오버 샘플링을 위해 각 열을 독립적으로 오버 샘플링 후 다시 합쳐주는 방식 선택\n",
    "\n",
    "- 하지만 지금까지는 실패~\n",
    "    - 각 열을 독립적으로 오버 샘플링 하다보니 각 열의 인덱스를 합쳐야 NA가 없음\n",
    "    - 그러나 랜덤으로 샘플링 하기 때문에 각 파일별 인덱스 수가 달라짐\n",
    "    - 따라서 망함 :,<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": [
     "not_use"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef discretize(value):\\n    if value == 0:\\n        return \\'Very Low\\'\\n    elif value == 0.25:\\n        return \\'Low\\'\\n    elif value == 0.5:\\n        return \\'Medium\\'\\n    elif value == 0.75:\\n        return \\'High\\'\\n    elif value == 1:\\n        return \\'Very High\\'\\n    return \\'Unknown\\'\\n\\ndef continuousize(category):\\n    mapping = {\\'Very Low\\': 0, \\'Low\\': 0.25, \\'Medium\\': 0.5, \\'High\\': 0.75, \\'Very High\\': 1}\\n    return mapping.get(category, -1)\\n\\n# 데이터 로드\\noriginal_data = pd.read_csv(f\\'{file_path}survey_replicated.csv\\')\\ncategories = [\\'work\\', \\'edu\\', \\'free_time\\', \\'health\\', \\'chores\\']\\n\\n# 범주화 및 원-핫 인코딩\\nX_encoded = pd.get_dummies(original_data.drop(categories, axis=1))\\nros = RandomOverSampler(random_state=42)\\n\\n# 각 범주에 대해 오버샘플링 적용 및 저장\\nfor category in categories:\\n    y = original_data[category].apply(discretize)\\n    X_resampled, y_resampled = ros.fit_resample(X_encoded, y)\\n    y_resampled = y_resampled.apply(continuousize)\\n    df_resampled = pd.DataFrame(X_resampled, columns=X_encoded.columns)\\n    df_resampled[category] = y_resampled\\n    df_resampled.to_csv(f\\'{file_path}survey_random_oversampled_{category}.csv\\', index=False)\\n    print(f\"오버샘플링 데이터 저장 완료: survey_random_oversampled_{category}.csv, 샘플 수: {len(df_resampled)}\")\\n\\nprint(\"모든 범주에 대한 오버샘플링 데이터 저장 완료\")\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메모리 사용을 줄이기 위해 범주에 대해 개별 오버 샘플링 후 저장\n",
    "'''\n",
    "def discretize(value):\n",
    "    if value == 0:\n",
    "        return 'Very Low'\n",
    "    elif value == 0.25:\n",
    "        return 'Low'\n",
    "    elif value == 0.5:\n",
    "        return 'Medium'\n",
    "    elif value == 0.75:\n",
    "        return 'High'\n",
    "    elif value == 1:\n",
    "        return 'Very High'\n",
    "    return 'Unknown'\n",
    "\n",
    "def continuousize(category):\n",
    "    mapping = {'Very Low': 0, 'Low': 0.25, 'Medium': 0.5, 'High': 0.75, 'Very High': 1}\n",
    "    return mapping.get(category, -1)\n",
    "\n",
    "# 데이터 로드\n",
    "original_data = pd.read_csv(f'{file_path}survey_replicated.csv')\n",
    "categories = ['work', 'edu', 'free_time', 'health', 'chores']\n",
    "\n",
    "# 범주화 및 원-핫 인코딩\n",
    "X_encoded = pd.get_dummies(original_data.drop(categories, axis=1))\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "# 각 범주에 대해 오버샘플링 적용 및 저장\n",
    "for category in categories:\n",
    "    y = original_data[category].apply(discretize)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X_encoded, y)\n",
    "    y_resampled = y_resampled.apply(continuousize)\n",
    "    df_resampled = pd.DataFrame(X_resampled, columns=X_encoded.columns)\n",
    "    df_resampled[category] = y_resampled\n",
    "    df_resampled.to_csv(f'{file_path}survey_random_oversampled_{category}.csv', index=False)\n",
    "    print(f\"오버샘플링 데이터 저장 완료: survey_random_oversampled_{category}.csv, 샘플 수: {len(df_resampled)}\")\n",
    "\n",
    "print(\"모든 범주에 대한 오버샘플링 데이터 저장 완료\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": [
     "not_use"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 데이터 로드 및 함수 정의\\ndef discretize(value):\\n    if value == 0:\\n        return \\'Very Low\\'\\n    elif value == 0.25:\\n        return \\'Low\\'\\n    elif value == 0.5:\\n        return \\'Medium\\'\\n    elif value == 0.75:\\n        return \\'High\\'\\n    elif value == 1:\\n        return \\'Very High\\'\\n    return \\'Unknown\\'\\n\\ndef continuousize(category):\\n    mapping = {\\'Very Low\\': 0, \\'Low\\': 0.25, \\'Medium\\': 0.5, \\'High\\': 0.75, \\'Very High\\': 1}\\n    return mapping.get(category, -1)\\n\\noriginal_data = pd.read_csv(f\\'{file_path}survey_replicated.csv\\')\\ncategories = [\\'work\\', \\'edu\\', \\'free_time\\', \\'health\\', \\'chores\\']\\n\\n# 범주화 및 원-핫 인코딩\\nX_encoded = pd.get_dummies(original_data.drop(categories, axis=1))\\noversampled_dataframes = []\\nros = RandomOverSampler(random_state=42)\\n\\n# 각 범주에 대해 오버샘플링 적용\\nfor category in categories:\\n    y = original_data[category].apply(discretize)\\n    X_resampled, y_resampled = ros.fit_resample(X_encoded, y)\\n    y_resampled = y_resampled.apply(continuousize)\\n    df_resampled = pd.DataFrame(X_resampled, columns=X_encoded.columns)\\n    df_resampled[category] = y_resampled\\n    oversampled_dataframes.append(df_resampled)\\n\\n# 모든 결과를 하나의 DataFrame으로 병합\\nfinal_df = oversampled_dataframes[0]\\nfor df in oversampled_dataframes[1:]:\\n    final_df = final_df.merge(df, on=list(X_encoded.columns), how=\\'inner\\')\\n\\n# 반복적 오버샘플링으로 추가 증가\\nfor _ in range(200):  # 반복 횟수에 따라 조정 가능\\n    final_df, _ = ros.fit_resample(final_df, final_df.columns)\\n\\n# 결과 저장\\nfinal_df.to_csv(f\\'{file_path}survey_random_oversampled.csv\\', index=False)\\nprint(f\"오버샘플링 데이터 저장 완료: survey_random_oversampled.csv, 샘플 수: {len(final_df)}\")\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모든 열에 처리시 메모리 사용량이 지나치게 높기에 위 코드를 통해 각 범주별 개별 오버샘플링 적용 후 합치는 방식 선택\n",
    "'''\n",
    "# 데이터 로드 및 함수 정의\n",
    "def discretize(value):\n",
    "    if value == 0:\n",
    "        return 'Very Low'\n",
    "    elif value == 0.25:\n",
    "        return 'Low'\n",
    "    elif value == 0.5:\n",
    "        return 'Medium'\n",
    "    elif value == 0.75:\n",
    "        return 'High'\n",
    "    elif value == 1:\n",
    "        return 'Very High'\n",
    "    return 'Unknown'\n",
    "\n",
    "def continuousize(category):\n",
    "    mapping = {'Very Low': 0, 'Low': 0.25, 'Medium': 0.5, 'High': 0.75, 'Very High': 1}\n",
    "    return mapping.get(category, -1)\n",
    "\n",
    "original_data = pd.read_csv(f'{file_path}survey_replicated.csv')\n",
    "categories = ['work', 'edu', 'free_time', 'health', 'chores']\n",
    "\n",
    "# 범주화 및 원-핫 인코딩\n",
    "X_encoded = pd.get_dummies(original_data.drop(categories, axis=1))\n",
    "oversampled_dataframes = []\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "# 각 범주에 대해 오버샘플링 적용\n",
    "for category in categories:\n",
    "    y = original_data[category].apply(discretize)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X_encoded, y)\n",
    "    y_resampled = y_resampled.apply(continuousize)\n",
    "    df_resampled = pd.DataFrame(X_resampled, columns=X_encoded.columns)\n",
    "    df_resampled[category] = y_resampled\n",
    "    oversampled_dataframes.append(df_resampled)\n",
    "\n",
    "# 모든 결과를 하나의 DataFrame으로 병합\n",
    "final_df = oversampled_dataframes[0]\n",
    "for df in oversampled_dataframes[1:]:\n",
    "    final_df = final_df.merge(df, on=list(X_encoded.columns), how='inner')\n",
    "\n",
    "# 반복적 오버샘플링으로 추가 증가\n",
    "for _ in range(200):  # 반복 횟수에 따라 조정 가능\n",
    "    final_df, _ = ros.fit_resample(final_df, final_df.columns)\n",
    "\n",
    "# 결과 저장\n",
    "final_df.to_csv(f'{file_path}survey_random_oversampled.csv', index=False)\n",
    "print(f\"오버샘플링 데이터 저장 완료: survey_random_oversampled.csv, 샘플 수: {len(final_df)}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": [
     "not_use"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport glob\\nimport os\\n\\n# 모든 CSV 파일의 경로를 리스트로 로드\\ncsv_files = glob.glob(os.path.join(file_path, \\'survey_random*.csv\\'))\\n\\n# CSV 파일 경로 출력 (디버깅용)\\nprint(\"CSV files found:\", csv_files)\\n\\n# CSV 파일이 존재하는지 확인\\nif not csv_files:\\n    raise ValueError(\"No CSV files found in the specified directory.\")\\n\\n# 모든 CSV 파일을 읽어 하나의 데이터프레임으로 병합\\ndataframes = []\\nfor file in csv_files:\\n    try:\\n        df = pd.read_csv(file)\\n        dataframes.append(df)\\n    except Exception as e:\\n        print(f\"Error reading {file}: {e}\")\\n\\n# 데이터프레임 병합\\nif dataframes:\\n    merged_dataframe = pd.concat(dataframes, ignore_index=True)\\n    # 합쳐진 데이터프레임을 새로운 CSV 파일로 저장\\n    merged_dataframe.to_csv(f\"{file_path}merged_survey_data.csv\", index=False)\\nelse:\\n    raise ValueError(\"No valid CSV files to concatenate.\")\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# 모든 CSV 파일의 경로를 리스트로 로드\n",
    "csv_files = glob.glob(os.path.join(file_path, 'survey_random*.csv'))\n",
    "\n",
    "# CSV 파일 경로 출력 (디버깅용)\n",
    "print(\"CSV files found:\", csv_files)\n",
    "\n",
    "# CSV 파일이 존재하는지 확인\n",
    "if not csv_files:\n",
    "    raise ValueError(\"No CSV files found in the specified directory.\")\n",
    "\n",
    "# 모든 CSV 파일을 읽어 하나의 데이터프레임으로 병합\n",
    "dataframes = []\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "# 데이터프레임 병합\n",
    "if dataframes:\n",
    "    merged_dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "    # 합쳐진 데이터프레임을 새로운 CSV 파일로 저장\n",
    "    merged_dataframe.to_csv(f\"{file_path}merged_survey_data.csv\", index=False)\n",
    "else:\n",
    "    raise ValueError(\"No valid CSV files to concatenate.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 랜덤 오버 샘플링을 통해 늘어난 데이터 수를 바탕으로 smote 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오버샘플링 데이터 저장 완료: survey_oversampled.csv\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "original_data = pd.read_csv(f'{file_path}survey_replicated.csv')\n",
    "\n",
    "# 데이터 스케일링 및 클러스터링\n",
    "scaler = StandardScaler()\n",
    "y_scaled = scaler.fit_transform(original_data[['work', 'edu', 'free_time', 'health', 'chores']])\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(y_scaled)\n",
    "\n",
    "\n",
    "# SMOTE 적용 (연속형 데이터 포함)\n",
    "# n_neighbors 값 조정: 가장 작은 클러스터의 크기보다 작게 설정\n",
    "min_cluster_size = min(pd.Series(clusters).value_counts())\n",
    "n_neighbors = max(min(2, min_cluster_size - 1), 1)  # 최소 1, 최대 (가장 작은 클러스터 크기 - 1) 사이\n",
    "smote = SMOTE(random_state=42, k_neighbors=n_neighbors)\n",
    "X_resampled, clusters_resampled = smote.fit_resample(original_data, clusters)\n",
    "\n",
    "# 연속형 데이터 복원: 각 클러스터의 중심을 사용하여 연속형 데이터를 복원\n",
    "cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "oversampled_y = [cluster_centers[cluster] for cluster in clusters_resampled]\n",
    "\n",
    "# 오버샘플링된 데이터 프레임 생성\n",
    "oversampled_df = pd.DataFrame(X_resampled, columns=original_data.columns)\n",
    "oversampled_df[['work', 'edu', 'free_time', 'health', 'chores']] = pd.DataFrame(oversampled_y, columns=['work', 'edu', 'free_time', 'health', 'chores'])\n",
    "\n",
    "# 결과 저장 및 출력\n",
    "oversampled_df.to_csv(f'{file_path}survey_oversampled.csv', index=False)\n",
    "print(\"오버샘플링 데이터 저장 완료: survey_oversampled.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 오버 샘플링 된 데이터들을 정제하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 오버샘플링된 데이터 로드\\noversampled_data = pd.read_csv(f\\'{file_path}survey_oversampled.csv\\')\\n\\n# \\'job\\' 관련 열 추출 및 변환\\njob_columns = [col for col in oversampled_data.columns if col.startswith(\\'job_\\')]\\noversampled_data[\\'job\\'] = oversampled_data[job_columns].idxmax(axis=1).apply(lambda x: x.split(\\'_\\')[1] if pd.notna(x) else None)\\n\\n# \\'mbti\\' 관련 열 추출 및 변환\\nmbti_columns = [col for col in oversampled_data.columns if col.startswith(\\'mbti_\\')]\\noversampled_data[\\'mbti\\'] = oversampled_data[mbti_columns].idxmax(axis=1).apply(lambda x: x.split(\\'_\\')[1] if pd.notna(x) else None)\\n\\n# 불필요한 원-핫 인코딩 열 제거\\noversampled_data.drop(columns=job_columns + mbti_columns, inplace=True)\\n\\n# 결과 저장\\noversampled_data.to_csv(f\\'{file_path}survey_oversampled_fixed.csv\\', index=False)\\n\\nprint(\"데이터 업데이트 완료: survey_oversampled_fixed.csv\")\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 오버샘플링된 데이터 로드\n",
    "oversampled_data = pd.read_csv(f'{file_path}survey_oversampled.csv')\n",
    "\n",
    "# 'job' 관련 열 추출 및 변환\n",
    "job_columns = [col for col in oversampled_data.columns if col.startswith('job_')]\n",
    "oversampled_data['job'] = oversampled_data[job_columns].idxmax(axis=1).apply(lambda x: x.split('_')[1] if pd.notna(x) else None)\n",
    "\n",
    "# 'mbti' 관련 열 추출 및 변환\n",
    "mbti_columns = [col for col in oversampled_data.columns if col.startswith('mbti_')]\n",
    "oversampled_data['mbti'] = oversampled_data[mbti_columns].idxmax(axis=1).apply(lambda x: x.split('_')[1] if pd.notna(x) else None)\n",
    "\n",
    "# 불필요한 원-핫 인코딩 열 제거\n",
    "oversampled_data.drop(columns=job_columns + mbti_columns, inplace=True)\n",
    "\n",
    "# 결과 저장\n",
    "oversampled_data.to_csv(f'{file_path}survey_oversampled_fixed.csv', index=False)\n",
    "\n",
    "print(\"데이터 업데이트 완료: survey_oversampled_fixed.csv\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막으로 value 수정\n",
    "fix_data = pd.read_csv(f'{file_path}survey_oversampled.csv')\n",
    "\n",
    "# 열 순서 재배열\n",
    "columns_ordered = ['age', 'gender', 'job', 'mbti', 'work', 'edu', 'free_time', 'health', 'chores']\n",
    "fix_data = fix_data[columns_ordered]\n",
    "\n",
    "fix_data.to_csv(f'{file_path}survey_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>job</th>\n",
       "      <th>mbti</th>\n",
       "      <th>work</th>\n",
       "      <th>edu</th>\n",
       "      <th>free_time</th>\n",
       "      <th>health</th>\n",
       "      <th>chores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>109095.000000</td>\n",
       "      <td>109095.000000</td>\n",
       "      <td>109095.000000</td>\n",
       "      <td>109095.000000</td>\n",
       "      <td>109095.000000</td>\n",
       "      <td>109095.000000</td>\n",
       "      <td>109095.000000</td>\n",
       "      <td>109095.000000</td>\n",
       "      <td>109095.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.457253</td>\n",
       "      <td>0.608085</td>\n",
       "      <td>4.375618</td>\n",
       "      <td>7.432036</td>\n",
       "      <td>0.638429</td>\n",
       "      <td>0.426707</td>\n",
       "      <td>0.477849</td>\n",
       "      <td>0.659269</td>\n",
       "      <td>0.205103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.433338</td>\n",
       "      <td>0.488180</td>\n",
       "      <td>1.580834</td>\n",
       "      <td>4.507275</td>\n",
       "      <td>0.191892</td>\n",
       "      <td>0.211148</td>\n",
       "      <td>0.249885</td>\n",
       "      <td>0.246948</td>\n",
       "      <td>0.202161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.368404</td>\n",
       "      <td>0.265633</td>\n",
       "      <td>0.124991</td>\n",
       "      <td>0.312485</td>\n",
       "      <td>0.012505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.368404</td>\n",
       "      <td>0.265633</td>\n",
       "      <td>0.124991</td>\n",
       "      <td>0.312485</td>\n",
       "      <td>0.012505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.750027</td>\n",
       "      <td>0.289492</td>\n",
       "      <td>0.637495</td>\n",
       "      <td>0.796889</td>\n",
       "      <td>0.118418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.796855</td>\n",
       "      <td>0.724997</td>\n",
       "      <td>0.671059</td>\n",
       "      <td>0.868432</td>\n",
       "      <td>0.484385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.796855</td>\n",
       "      <td>0.724997</td>\n",
       "      <td>0.671059</td>\n",
       "      <td>0.868432</td>\n",
       "      <td>0.484385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 age         gender            job           mbti  \\\n",
       "count  109095.000000  109095.000000  109095.000000  109095.000000   \n",
       "mean       29.457253       0.608085       4.375618       7.432036   \n",
       "std         7.433338       0.488180       1.580834       4.507275   \n",
       "min        14.000000       0.000000       0.000000       0.000000   \n",
       "25%        25.000000       0.000000       3.000000       4.000000   \n",
       "50%        28.000000       1.000000       4.000000       7.000000   \n",
       "75%        34.000000       1.000000       5.000000      12.000000   \n",
       "max        45.000000       1.000000       8.000000      15.000000   \n",
       "\n",
       "                work            edu      free_time         health  \\\n",
       "count  109095.000000  109095.000000  109095.000000  109095.000000   \n",
       "mean        0.638429       0.426707       0.477849       0.659269   \n",
       "std         0.191892       0.211148       0.249885       0.246948   \n",
       "min         0.368404       0.265633       0.124991       0.312485   \n",
       "25%         0.368404       0.265633       0.124991       0.312485   \n",
       "50%         0.750027       0.289492       0.637495       0.796889   \n",
       "75%         0.796855       0.724997       0.671059       0.868432   \n",
       "max         0.796855       0.724997       0.671059       0.868432   \n",
       "\n",
       "              chores  \n",
       "count  109095.000000  \n",
       "mean        0.205103  \n",
       "std         0.202161  \n",
       "min         0.012505  \n",
       "25%         0.012505  \n",
       "50%         0.118418  \n",
       "75%         0.484385  \n",
       "max         0.484385  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터를 DB에 저장하기 위해 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './survey_oversampled_fixed.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./survey_oversampled_fixed.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# CSV 파일 읽기\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 나이 범위를 age_range 컬럼으로 추가\u001b[39;00m\n\u001b[0;32m      8\u001b[0m bins \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m100\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './survey_oversampled_fixed.csv'"
     ]
    }
   ],
   "source": [
    "# 파일 경로\n",
    "path = './survey_oversampled_fixed.csv'\n",
    "\n",
    "# CSV 파일 읽기\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# 나이 범위를 age_range 컬럼으로 추가\n",
    "bins = [0, 20, 25, 30, 100]\n",
    "labels = ['0-19', '20-24', '25-29', '30-']\n",
    "data['age_range'] = pd.cut(data['age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# age_range 행을 기준으로 분류하여 각 변수의 가중치 평균 계산\n",
    "grouped_means = data.groupby('age_range')[['work', 'edu', 'free_time', 'health', 'chores']].mean()\n",
    "\n",
    "# 결과 출력\n",
    "grouped_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
