{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설문 조사 데이터를 오버샘플링 하기 위한 코드\n",
    "- 샘플링 수를 늘려보자 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 필요한 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 랜덤 오버 샘플링을 위한 라이브러리\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파일 위치 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './refer/output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 설문 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def survey_processing(file_path = f'{file_path}survey.csv', output_path = f'{file_path}survey_preprocessed.csv'):\n",
    "    # CSV 파일을 불러옵니다.\n",
    "    survey_df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 우선순위에 대한 가중치 매핑을 정의합니다.\n",
    "    priority_weights = {\n",
    "        \"건강\": \"health\",\n",
    "        \"여가시간\": \"free_time\",\n",
    "        \"학업 및 자기계발\": \"edu\",\n",
    "        \"업무\": \"work\",\n",
    "        \"집안일\": \"chores\",\n",
    "        None: \"category_else\"\n",
    "    }\n",
    "    \n",
    "    # 각 우선순위 컬럼에 대해 가중치를 적용합니다.\n",
    "    for i in range(1, 6):\n",
    "        column_name = f\"본인이 생각하는 일과 별 중요도를 우선순위를 정하여 체크해주세요. (각 순위 별로 하나씩만 체크해주세요) [{i} 순위]\"\n",
    "        survey_df[column_name] = survey_df[column_name].map(priority_weights)\n",
    "    \n",
    "    # 필요한 컬럼들만 선택하고 리네임합니다.\n",
    "    columns_to_keep = [\n",
    "        \"설문자의 나이(만)는 어떻게 되십니까?\",\n",
    "        \"설문자의 해당사항을 체크해주세요.\",\n",
    "        \"설문자의 MBTI는 무엇입니까?\",\n",
    "        \"설문자의 성별은 어떻게 되십니까?\",\n",
    "        \"본인이 생각하는 일과 별 중요도를 우선순위를 정하여 체크해주세요. (각 순위 별로 하나씩만 체크해주세요) [1 순위]\",\n",
    "        \"본인이 생각하는 일과 별 중요도를 우선순위를 정하여 체크해주세요. (각 순위 별로 하나씩만 체크해주세요) [2 순위]\",\n",
    "        \"본인이 생각하는 일과 별 중요도를 우선순위를 정하여 체크해주세요. (각 순위 별로 하나씩만 체크해주세요) [3 순위]\",\n",
    "        \"본인이 생각하는 일과 별 중요도를 우선순위를 정하여 체크해주세요. (각 순위 별로 하나씩만 체크해주세요) [4 순위]\",\n",
    "        \"본인이 생각하는 일과 별 중요도를 우선순위를 정하여 체크해주세요. (각 순위 별로 하나씩만 체크해주세요) [5 순위]\"\n",
    "    ]\n",
    "    \n",
    "    survey_df = survey_df[columns_to_keep]\n",
    "    \n",
    "    # 컬럼명을 변환합니다.\n",
    "    column_mapping = {\n",
    "        \"설문자의 나이(만)는 어떻게 되십니까?\": \"age\",\n",
    "        \"설문자의 해당사항을 체크해주세요.\": \"job\",\n",
    "        \"설문자의 MBTI는 무엇입니까?\": \"mbti\",\n",
    "        \"설문자의 성별은 어떻게 되십니까?\": \"gender\",\n",
    "        \"본인이 생각하는 일과 별 중요도를 우선순위를 정하여 체크해주세요. (각 순위 별로 하나씩만 체크해주세요) [1 순위]\": \"priority_1\",\n",
    "        \"본인이 생각하는 일과 별 중요도를 우선순위를 정하여 체크해주세요. (각 순위 별로 하나씩만 체크해주세요) [2 순위]\": \"priority_2\",\n",
    "        \"본인이 생각하는 일과 별 중요도를 우선순위를 정하여 체크해주세요. (각 순위 별로 하나씩만 체크해주세요) [3 순위]\": \"priority_3\",\n",
    "        \"본인이 생각하는 일과 별 중요도를 우선순위를 정하여 체크해주세요. (각 순위 별로 하나씩만 체크해주세요) [4 순위]\": \"priority_4\",\n",
    "        \"본인이 생각하는 일과 별 중요도를 우선순위를 정하여 체크해주세요. (각 순위 별로 하나씩만 체크해주세요) [5 순위]\": \"priority_5\"\n",
    "    }\n",
    "    \n",
    "    survey_df.rename(columns=column_mapping, inplace=True)\n",
    "    \n",
    "    # 새로운 컬럼 'category_else'를 추가하고 모든 값을 0으로 설정합니다.\n",
    "    survey_df[\"category_else\"] = 0\n",
    "    \n",
    "    # 나이, 직업, MBTI 컬럼을 숫자 코드로 변환합니다.\n",
    "    age_mapping = {\n",
    "        \"0~14세\": (0, 14),\n",
    "        \"15~19세\": (15, 19),\n",
    "        \"20~24세\": (20, 24),\n",
    "        \"25~30세\": (25, 30),\n",
    "        \"31세 이상\": (31, 45)\n",
    "    }\n",
    "    job_mapping = {\n",
    "        \"초/중학생\": \"000\",\n",
    "        \"고등학생\": \"001\",\n",
    "        \"대학생 / 저학년 (1-2학년)\": \"002\",\n",
    "        \"대학생 / 고학년(3-4학년)\": \"003\",\n",
    "        \"구직자\": \"004\",\n",
    "        \"직장인\": \"005\",\n",
    "        \"자영업자\": \"006\",\n",
    "        \"프리랜서\": \"007\",\n",
    "        \"주부\": \"008\",\n",
    "        \"기타\": \"009\"\n",
    "    }\n",
    "    mbti_mapping = {\n",
    "        \"INTJ\": \"00\",\n",
    "        \"INTP\": \"01\",\n",
    "        \"ENTJ\": \"02\",\n",
    "        \"ENTP\": \"03\",\n",
    "        \"INFJ\": \"04\",\n",
    "        \"INFP\": \"05\",\n",
    "        \"ENFJ\": \"06\",\n",
    "        \"ENFP\": \"07\",\n",
    "        \"ISTJ\": \"08\",\n",
    "        \"ISFJ\": \"09\",\n",
    "        \"ESTJ\": \"10\",\n",
    "        \"ESFJ\": \"11\",\n",
    "        \"ISTP\": \"12\",\n",
    "        \"ISFP\": \"13\",\n",
    "        \"ESTP\": \"14\",\n",
    "        \"ESFP\": \"15\"\n",
    "    }\n",
    "    gender_mapping = {\n",
    "        \"남\": 0,\n",
    "        \"여\": 1\n",
    "    }\n",
    "\n",
    "    # 나이, 직업, MBTI, 성별 컬럼을 숫자 코드로 변환합니다.\n",
    "    survey_df['age'] = survey_df['age'].map(lambda x: np.random.randint(age_mapping[x][0], age_mapping[x][1] + 1))\n",
    "    survey_df['job'] = survey_df['job'].map(job_mapping)\n",
    "    survey_df['mbti'] = survey_df['mbti'].str.upper().map(mbti_mapping)\n",
    "    survey_df['gender'] = survey_df['gender'].map(gender_mapping)\n",
    "\n",
    "    # 우선순위별로 각 항목에 가중치를 적용한 값을 할당합니다.\n",
    "    work_col = []\n",
    "    edu_col = []\n",
    "    free_time_col = []\n",
    "    health_col = []\n",
    "    chores_col = []\n",
    "\n",
    "    for _, row in survey_df.iterrows():\n",
    "        work = 0\n",
    "        edu = 0\n",
    "        free_time = 0\n",
    "        health = 0\n",
    "        chores = 0\n",
    "\n",
    "        for i in range(1, 6):\n",
    "            if row[f'priority_{i}'] == 'work':\n",
    "                work = 1 - (i - 1) * 0.25\n",
    "            elif row[f'priority_{i}'] == 'edu':\n",
    "                edu = 1 - (i - 1) * 0.25\n",
    "            elif row[f'priority_{i}'] == 'free_time':\n",
    "                free_time = 1 - (i - 1) * 0.25\n",
    "            elif row[f'priority_{i}'] == 'health':\n",
    "                health = 1 - (i - 1) * 0.25\n",
    "            elif row[f'priority_{i}'] == 'chores':\n",
    "                chores = 1 - (i - 1) * 0.25\n",
    "        \n",
    "        work_col.append(work)\n",
    "        edu_col.append(edu)\n",
    "        free_time_col.append(free_time)\n",
    "        health_col.append(health)\n",
    "        chores_col.append(chores)\n",
    "    \n",
    "    survey_df['work'] = work_col\n",
    "    survey_df['edu'] = edu_col\n",
    "    survey_df['free_time'] = free_time_col\n",
    "    survey_df['health'] = health_col\n",
    "    survey_df['chores'] = chores_col\n",
    "    \n",
    "    # 사용하지 않는 우선순위 컬럼 삭제\n",
    "    survey_df.drop(columns=['priority_1', 'priority_2', 'priority_3', 'priority_4', 'priority_5'], inplace=True)\n",
    "    \n",
    "    # 컬럼 순서 재정렬\n",
    "    survey_df = survey_df[['age', 'job', 'mbti', 'gender', 'work', 'edu', 'free_time', 'health', 'chores', 'category_else']]\n",
    "    \n",
    "    # 수정된 데이터프레임을 새로운 CSV 파일로 저장합니다.\n",
    "    survey_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_processing()  # 함수를 호출하여 변환을 수행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 원본 데이터 단순 복제로 데이터 수를 50만개로 늘리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 복제 완료: 총 샘플 수 500000\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "data = pd.read_csv(f'{file_path}survey_preprocessed.csv')\n",
    "\n",
    "# 원하는 샘플 수\n",
    "target_samples = 500000\n",
    "replication_factor = target_samples // len(data)  # 필요한 복제 횟수 계산\n",
    "additional_samples = target_samples % len(data)   # 추가로 필요한 샘플 수\n",
    "\n",
    "# 데이터 복제\n",
    "oversampled_data = pd.concat([data] * replication_factor + [data.iloc[:additional_samples]])\n",
    "\n",
    "# 결과 저장\n",
    "oversampled_data.to_csv(f'{file_path}survey_replicated.csv', index=False)\n",
    "\n",
    "print(f\"데이터 복제 완료: 총 샘플 수 {len(oversampled_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 랜덤 오버 샘플링을 통해 전체 데이터의 수를 늘리기\n",
    "\n",
    "- 단, 랜덤 오버 샘플링은 이산형 데이터에 사용이 가능\n",
    "- 따라서 '0', '0.25', '0.5', '0.75', '1'을 범주화하여 제한을 우회\n",
    "- 이후 다시 범주형 데이터를 수치로 롤백하여 원본 데이터의 형식을 유지\n",
    "- 이 외의 숫자가 혹시 포함되어 있다면 디버깅을 위해 -1로 설정\n",
    "\n",
    "- 랜덤 오버 샘플링을 위해 각 열을 독립적으로 오버 샘플링 후 다시 합쳐주는 방식 선택\n",
    "\n",
    "- 하지만 지금까지는 실패~\n",
    "    - 각 열을 독립적으로 오버 샘플링 하다보니 각 열의 인덱스를 합쳐야 NA가 없음\n",
    "    - 그러나 랜덤으로 샘플링 하기 때문에 각 파일별 인덱스 수가 달라짐\n",
    "    - 따라서 망함 :,<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "not_use"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef discretize(value):\\n    if value == 0:\\n        return \\'Very Low\\'\\n    elif value == 0.25:\\n        return \\'Low\\'\\n    elif value == 0.5:\\n        return \\'Medium\\'\\n    elif value == 0.75:\\n        return \\'High\\'\\n    elif value == 1:\\n        return \\'Very High\\'\\n    return \\'Unknown\\'\\n\\ndef continuousize(category):\\n    mapping = {\\'Very Low\\': 0, \\'Low\\': 0.25, \\'Medium\\': 0.5, \\'High\\': 0.75, \\'Very High\\': 1}\\n    return mapping.get(category, -1)\\n\\n# 데이터 로드\\noriginal_data = pd.read_csv(f\\'{file_path}survey_replicated.csv\\')\\ncategories = [\\'work\\', \\'edu\\', \\'free_time\\', \\'health\\', \\'chores\\']\\n\\n# 범주화 및 원-핫 인코딩\\nX_encoded = pd.get_dummies(original_data.drop(categories, axis=1))\\nros = RandomOverSampler(random_state=42)\\n\\n# 각 범주에 대해 오버샘플링 적용 및 저장\\nfor category in categories:\\n    y = original_data[category].apply(discretize)\\n    X_resampled, y_resampled = ros.fit_resample(X_encoded, y)\\n    y_resampled = y_resampled.apply(continuousize)\\n    df_resampled = pd.DataFrame(X_resampled, columns=X_encoded.columns)\\n    df_resampled[category] = y_resampled\\n    df_resampled.to_csv(f\\'{file_path}survey_random_oversampled_{category}.csv\\', index=False)\\n    print(f\"오버샘플링 데이터 저장 완료: survey_random_oversampled_{category}.csv, 샘플 수: {len(df_resampled)}\")\\n\\nprint(\"모든 범주에 대한 오버샘플링 데이터 저장 완료\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메모리 사용을 줄이기 위해 범주에 대해 개별 오버 샘플링 후 저장\n",
    "'''\n",
    "def discretize(value):\n",
    "    if value == 0:\n",
    "        return 'Very Low'\n",
    "    elif value == 0.25:\n",
    "        return 'Low'\n",
    "    elif value == 0.5:\n",
    "        return 'Medium'\n",
    "    elif value == 0.75:\n",
    "        return 'High'\n",
    "    elif value == 1:\n",
    "        return 'Very High'\n",
    "    return 'Unknown'\n",
    "\n",
    "def continuousize(category):\n",
    "    mapping = {'Very Low': 0, 'Low': 0.25, 'Medium': 0.5, 'High': 0.75, 'Very High': 1}\n",
    "    return mapping.get(category, -1)\n",
    "\n",
    "# 데이터 로드\n",
    "original_data = pd.read_csv(f'{file_path}survey_replicated.csv')\n",
    "categories = ['work', 'edu', 'free_time', 'health', 'chores']\n",
    "\n",
    "# 범주화 및 원-핫 인코딩\n",
    "X_encoded = pd.get_dummies(original_data.drop(categories, axis=1))\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "# 각 범주에 대해 오버샘플링 적용 및 저장\n",
    "for category in categories:\n",
    "    y = original_data[category].apply(discretize)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X_encoded, y)\n",
    "    y_resampled = y_resampled.apply(continuousize)\n",
    "    df_resampled = pd.DataFrame(X_resampled, columns=X_encoded.columns)\n",
    "    df_resampled[category] = y_resampled\n",
    "    df_resampled.to_csv(f'{file_path}survey_random_oversampled_{category}.csv', index=False)\n",
    "    print(f\"오버샘플링 데이터 저장 완료: survey_random_oversampled_{category}.csv, 샘플 수: {len(df_resampled)}\")\n",
    "\n",
    "print(\"모든 범주에 대한 오버샘플링 데이터 저장 완료\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "not_use"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 데이터 로드 및 함수 정의\\ndef discretize(value):\\n    if value == 0:\\n        return \\'Very Low\\'\\n    elif value == 0.25:\\n        return \\'Low\\'\\n    elif value == 0.5:\\n        return \\'Medium\\'\\n    elif value == 0.75:\\n        return \\'High\\'\\n    elif value == 1:\\n        return \\'Very High\\'\\n    return \\'Unknown\\'\\n\\ndef continuousize(category):\\n    mapping = {\\'Very Low\\': 0, \\'Low\\': 0.25, \\'Medium\\': 0.5, \\'High\\': 0.75, \\'Very High\\': 1}\\n    return mapping.get(category, -1)\\n\\noriginal_data = pd.read_csv(f\\'{file_path}survey_replicated.csv\\')\\ncategories = [\\'work\\', \\'edu\\', \\'free_time\\', \\'health\\', \\'chores\\']\\n\\n# 범주화 및 원-핫 인코딩\\nX_encoded = pd.get_dummies(original_data.drop(categories, axis=1))\\noversampled_dataframes = []\\nros = RandomOverSampler(random_state=42)\\n\\n# 각 범주에 대해 오버샘플링 적용\\nfor category in categories:\\n    y = original_data[category].apply(discretize)\\n    X_resampled, y_resampled = ros.fit_resample(X_encoded, y)\\n    y_resampled = y_resampled.apply(continuousize)\\n    df_resampled = pd.DataFrame(X_resampled, columns=X_encoded.columns)\\n    df_resampled[category] = y_resampled\\n    oversampled_dataframes.append(df_resampled)\\n\\n# 모든 결과를 하나의 DataFrame으로 병합\\nfinal_df = oversampled_dataframes[0]\\nfor df in oversampled_dataframes[1:]:\\n    final_df = final_df.merge(df, on=list(X_encoded.columns), how=\\'inner\\')\\n\\n# 반복적 오버샘플링으로 추가 증가\\nfor _ in range(200):  # 반복 횟수에 따라 조정 가능\\n    final_df, _ = ros.fit_resample(final_df, final_df.columns)\\n\\n# 결과 저장\\nfinal_df.to_csv(f\\'{file_path}survey_random_oversampled.csv\\', index=False)\\nprint(f\"오버샘플링 데이터 저장 완료: survey_random_oversampled.csv, 샘플 수: {len(final_df)}\")\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모든 열에 처리시 메모리 사용량이 지나치게 높기에 위 코드를 통해 각 범주별 개별 오버샘플링 적용 후 합치는 방식 선택\n",
    "'''\n",
    "# 데이터 로드 및 함수 정의\n",
    "def discretize(value):\n",
    "    if value == 0:\n",
    "        return 'Very Low'\n",
    "    elif value == 0.25:\n",
    "        return 'Low'\n",
    "    elif value == 0.5:\n",
    "        return 'Medium'\n",
    "    elif value == 0.75:\n",
    "        return 'High'\n",
    "    elif value == 1:\n",
    "        return 'Very High'\n",
    "    return 'Unknown'\n",
    "\n",
    "def continuousize(category):\n",
    "    mapping = {'Very Low': 0, 'Low': 0.25, 'Medium': 0.5, 'High': 0.75, 'Very High': 1}\n",
    "    return mapping.get(category, -1)\n",
    "\n",
    "original_data = pd.read_csv(f'{file_path}survey_replicated.csv')\n",
    "categories = ['work', 'edu', 'free_time', 'health', 'chores']\n",
    "\n",
    "# 범주화 및 원-핫 인코딩\n",
    "X_encoded = pd.get_dummies(original_data.drop(categories, axis=1))\n",
    "oversampled_dataframes = []\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "# 각 범주에 대해 오버샘플링 적용\n",
    "for category in categories:\n",
    "    y = original_data[category].apply(discretize)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X_encoded, y)\n",
    "    y_resampled = y_resampled.apply(continuousize)\n",
    "    df_resampled = pd.DataFrame(X_resampled, columns=X_encoded.columns)\n",
    "    df_resampled[category] = y_resampled\n",
    "    oversampled_dataframes.append(df_resampled)\n",
    "\n",
    "# 모든 결과를 하나의 DataFrame으로 병합\n",
    "final_df = oversampled_dataframes[0]\n",
    "for df in oversampled_dataframes[1:]:\n",
    "    final_df = final_df.merge(df, on=list(X_encoded.columns), how='inner')\n",
    "\n",
    "# 반복적 오버샘플링으로 추가 증가\n",
    "for _ in range(200):  # 반복 횟수에 따라 조정 가능\n",
    "    final_df, _ = ros.fit_resample(final_df, final_df.columns)\n",
    "\n",
    "# 결과 저장\n",
    "final_df.to_csv(f'{file_path}survey_random_oversampled.csv', index=False)\n",
    "print(f\"오버샘플링 데이터 저장 완료: survey_random_oversampled.csv, 샘플 수: {len(final_df)}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "not_use"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport glob\\nimport os\\n\\n# 모든 CSV 파일의 경로를 리스트로 로드\\ncsv_files = glob.glob(os.path.join(file_path, \\'survey_random*.csv\\'))\\n\\n# CSV 파일 경로 출력 (디버깅용)\\nprint(\"CSV files found:\", csv_files)\\n\\n# CSV 파일이 존재하는지 확인\\nif not csv_files:\\n    raise ValueError(\"No CSV files found in the specified directory.\")\\n\\n# 모든 CSV 파일을 읽어 하나의 데이터프레임으로 병합\\ndataframes = []\\nfor file in csv_files:\\n    try:\\n        df = pd.read_csv(file)\\n        dataframes.append(df)\\n    except Exception as e:\\n        print(f\"Error reading {file}: {e}\")\\n\\n# 데이터프레임 병합\\nif dataframes:\\n    merged_dataframe = pd.concat(dataframes, ignore_index=True)\\n    # 합쳐진 데이터프레임을 새로운 CSV 파일로 저장\\n    merged_dataframe.to_csv(f\"{file_path}merged_survey_data.csv\", index=False)\\nelse:\\n    raise ValueError(\"No valid CSV files to concatenate.\")\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# 모든 CSV 파일의 경로를 리스트로 로드\n",
    "csv_files = glob.glob(os.path.join(file_path, 'survey_random*.csv'))\n",
    "\n",
    "# CSV 파일 경로 출력 (디버깅용)\n",
    "print(\"CSV files found:\", csv_files)\n",
    "\n",
    "# CSV 파일이 존재하는지 확인\n",
    "if not csv_files:\n",
    "    raise ValueError(\"No CSV files found in the specified directory.\")\n",
    "\n",
    "# 모든 CSV 파일을 읽어 하나의 데이터프레임으로 병합\n",
    "dataframes = []\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "# 데이터프레임 병합\n",
    "if dataframes:\n",
    "    merged_dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "    # 합쳐진 데이터프레임을 새로운 CSV 파일로 저장\n",
    "    merged_dataframe.to_csv(f\"{file_path}merged_survey_data.csv\", index=False)\n",
    "else:\n",
    "    raise ValueError(\"No valid CSV files to concatenate.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단순 오버 샘플링을 통해 늘어난 데이터 수를 바탕으로 smote 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "original_data = pd.read_csv(f'{file_path}survey_replicated.csv')\n",
    "\n",
    "# 데이터 스케일링 및 클러스터링\n",
    "scaler = StandardScaler()\n",
    "y_scaled = scaler.fit_transform(original_data[['work', 'edu', 'free_time', 'health', 'chores']])\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(y_scaled)\n",
    "\n",
    "\n",
    "# SMOTE 적용 (연속형 데이터 포함)\n",
    "# n_neighbors 값 조정: 가장 작은 클러스터의 크기보다 작게 설정\n",
    "min_cluster_size = min(pd.Series(clusters).value_counts())\n",
    "n_neighbors = max(min(2, min_cluster_size - 1), 1)  # 최소 1, 최대 (가장 작은 클러스터 크기 - 1) 사이\n",
    "smote = SMOTE(random_state=42, k_neighbors=n_neighbors)\n",
    "X_resampled, clusters_resampled = smote.fit_resample(original_data, clusters)\n",
    "\n",
    "# 연속형 데이터 복원: 각 클러스터의 중심을 사용하여 연속형 데이터를 복원\n",
    "cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "oversampled_y = [cluster_centers[cluster] for cluster in clusters_resampled]\n",
    "\n",
    "# 오버샘플링된 데이터 프레임 생성\n",
    "oversampled_df = pd.DataFrame(X_resampled, columns=original_data.columns)\n",
    "oversampled_df[['work', 'edu', 'free_time', 'health', 'chores']] = pd.DataFrame(oversampled_y, columns=['work', 'edu', 'free_time', 'health', 'chores'])\n",
    "\n",
    "# 결과 저장 및 출력\n",
    "oversampled_df.to_csv(f'{file_path}survey_oversampled.csv', index=False)\n",
    "print(\"오버샘플링 데이터 저장 완료: survey_oversampled.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 오버 샘플링 된 데이터들을 정제하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 오버샘플링된 데이터 로드\\noversampled_data = pd.read_csv(f\\'{file_path}survey_oversampled.csv\\')\\n\\n# \\'job\\' 관련 열 추출 및 변환\\njob_columns = [col for col in oversampled_data.columns if col.startswith(\\'job_\\')]\\noversampled_data[\\'job\\'] = oversampled_data[job_columns].idxmax(axis=1).apply(lambda x: x.split(\\'_\\')[1] if pd.notna(x) else None)\\n\\n# \\'mbti\\' 관련 열 추출 및 변환\\nmbti_columns = [col for col in oversampled_data.columns if col.startswith(\\'mbti_\\')]\\noversampled_data[\\'mbti\\'] = oversampled_data[mbti_columns].idxmax(axis=1).apply(lambda x: x.split(\\'_\\')[1] if pd.notna(x) else None)\\n\\n# 불필요한 원-핫 인코딩 열 제거\\noversampled_data.drop(columns=job_columns + mbti_columns, inplace=True)\\n\\n# 결과 저장\\noversampled_data.to_csv(f\\'{file_path}survey_oversampled_fixed.csv\\', index=False)\\n\\nprint(\"데이터 업데이트 완료: survey_oversampled_fixed.csv\")\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 오버샘플링된 데이터 로드\n",
    "oversampled_data = pd.read_csv(f'{file_path}survey_oversampled.csv')\n",
    "\n",
    "# 'job' 관련 열 추출 및 변환\n",
    "job_columns = [col for col in oversampled_data.columns if col.startswith('job_')]\n",
    "oversampled_data['job'] = oversampled_data[job_columns].idxmax(axis=1).apply(lambda x: x.split('_')[1] if pd.notna(x) else None)\n",
    "\n",
    "# 'mbti' 관련 열 추출 및 변환\n",
    "mbti_columns = [col for col in oversampled_data.columns if col.startswith('mbti_')]\n",
    "oversampled_data['mbti'] = oversampled_data[mbti_columns].idxmax(axis=1).apply(lambda x: x.split('_')[1] if pd.notna(x) else None)\n",
    "\n",
    "# 불필요한 원-핫 인코딩 열 제거\n",
    "oversampled_data.drop(columns=job_columns + mbti_columns, inplace=True)\n",
    "\n",
    "# 결과 저장\n",
    "oversampled_data.to_csv(f'{file_path}survey_oversampled_fixed.csv', index=False)\n",
    "\n",
    "print(\"데이터 업데이트 완료: survey_oversampled_fixed.csv\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 마지막으로 value 수정\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m fix_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124msurvey_oversampled.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 열 순서 재배열\u001b[39;00m\n\u001b[0;32m      5\u001b[0m columns_ordered \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgender\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmbti\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwork\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124medu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfree_time\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhealth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchores\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory_else\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# 마지막으로 value 수정\n",
    "fix_data = pd.read_csv(f'{file_path}survey_oversampled.csv')\n",
    "\n",
    "# 열 순서 재배열\n",
    "columns_ordered = ['age', 'gender', 'job', 'mbti', 'work', 'edu', 'free_time', 'health', 'chores']\n",
    "fix_data = fix_data[columns_ordered]\n",
    "\n",
    "fix_data.to_csv(f'{file_path}survey_data.csv')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>job</th>\n",
       "      <th>mbti</th>\n",
       "      <th>work</th>\n",
       "      <th>edu</th>\n",
       "      <th>free_time</th>\n",
       "      <th>health</th>\n",
       "      <th>chores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>600003.000000</td>\n",
       "      <td>600003.000000</td>\n",
       "      <td>600003.000000</td>\n",
       "      <td>600003.000000</td>\n",
       "      <td>600003.000000</td>\n",
       "      <td>600003.000000</td>\n",
       "      <td>600003.000000</td>\n",
       "      <td>600003.000000</td>\n",
       "      <td>600003.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>28.799424</td>\n",
       "      <td>0.589380</td>\n",
       "      <td>4.347768</td>\n",
       "      <td>7.354457</td>\n",
       "      <td>0.599243</td>\n",
       "      <td>0.422304</td>\n",
       "      <td>0.496634</td>\n",
       "      <td>0.684763</td>\n",
       "      <td>0.196383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.351794</td>\n",
       "      <td>0.491947</td>\n",
       "      <td>1.546530</td>\n",
       "      <td>4.525648</td>\n",
       "      <td>0.258853</td>\n",
       "      <td>0.157070</td>\n",
       "      <td>0.243146</td>\n",
       "      <td>0.262233</td>\n",
       "      <td>0.182342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.263885</td>\n",
       "      <td>0.152780</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.011364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.263885</td>\n",
       "      <td>0.152780</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.011364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.772729</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.819440</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.791668</td>\n",
       "      <td>0.636362</td>\n",
       "      <td>0.670454</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.444452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.791668</td>\n",
       "      <td>0.636362</td>\n",
       "      <td>0.670454</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.444452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 age         gender            job           mbti  \\\n",
       "count  600003.000000  600003.000000  600003.000000  600003.000000   \n",
       "mean       28.799424       0.589380       4.347768       7.354457   \n",
       "std         7.351794       0.491947       1.546530       4.525648   \n",
       "min        12.000000       0.000000       0.000000       0.000000   \n",
       "25%        25.000000       0.000000       3.000000       4.000000   \n",
       "50%        27.000000       1.000000       4.000000       7.000000   \n",
       "75%        32.000000       1.000000       5.000000      12.000000   \n",
       "max        45.000000       1.000000       8.000000      15.000000   \n",
       "\n",
       "                work            edu      free_time         health  \\\n",
       "count  600003.000000  600003.000000  600003.000000  600003.000000   \n",
       "mean        0.599243       0.422304       0.496634       0.684763   \n",
       "std         0.258853       0.157070       0.243146       0.262233   \n",
       "min         0.233333       0.263885       0.152780       0.318182   \n",
       "25%         0.233333       0.263885       0.152780       0.318182   \n",
       "50%         0.772729       0.366667       0.666667       0.819440   \n",
       "75%         0.791668       0.636362       0.670454       0.916667   \n",
       "max         0.791668       0.636362       0.670454       0.916667   \n",
       "\n",
       "              chores  \n",
       "count  600003.000000  \n",
       "mean        0.196383  \n",
       "std         0.182342  \n",
       "min         0.011364  \n",
       "25%         0.011364  \n",
       "50%         0.133333  \n",
       "75%         0.444452  \n",
       "max         0.444452  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
